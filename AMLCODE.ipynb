{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CPMC and SCGP algorithm implementation\n"
      ],
      "metadata": {
        "id": "HaAhPecj1bUQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxvqLHaH1Xhe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CPMC ALGO"
      ],
      "metadata": {
        "id": "g0xOOHzz3RXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import concurrent.futures\n",
        "from skimage import segmentation\n",
        "import os\n",
        "\n",
        "def generate_superpixels(img, num_segments=100, compactness=30):\n",
        "    \"\"\"\n",
        "    Generate superpixels using scikit-image's SLIC.\n",
        "    \"\"\"\n",
        "    img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "    segments = segmentation.slic(img_lab, n_segments=num_segments, compactness=compactness, start_label=1)\n",
        "    return segments\n",
        "\n",
        "def compute_proposal_grabcut(img, segment_mask, global_foreground_mask, iter_count=2):\n",
        "    \"\"\"\n",
        "    Run GrabCut on a given superpixel (mask) with a specified number of iterations.\n",
        "    The global_foreground_mask ensures that already detected foreground is not included again.\n",
        "    \"\"\"\n",
        "    mask = np.full(img.shape[:2], cv2.GC_BGD, dtype=np.uint8)\n",
        "    mask[segment_mask > 0] = cv2.GC_PR_FGD\n",
        "\n",
        "    # Ensure previously detected foreground is marked as background\n",
        "    mask[global_foreground_mask > 0] = cv2.GC_BGD\n",
        "\n",
        "    bgdModel = np.zeros((1, 65), np.float64)\n",
        "    fgdModel = np.zeros((1, 65), np.float64)\n",
        "\n",
        "    cv2.grabCut(img, mask, None, bgdModel, fgdModel, iter_count, cv2.GC_INIT_WITH_MASK)\n",
        "    proposal = np.where((mask == cv2.GC_FGD) | (mask == cv2.GC_PR_FGD), 1, 0).astype(np.uint8)\n",
        "\n",
        "    # Update global foreground mask (accumulate detected foreground regions)\n",
        "    global_foreground_mask[proposal > 0] = 1\n",
        "\n",
        "    return proposal\n",
        "\n",
        "def process_task(args):\n",
        "    img, segment, global_foreground_mask, iter_count = args\n",
        "    return compute_proposal_grabcut(img, segment, global_foreground_mask, iter_count)\n",
        "\n",
        "def cpmc_object_proposals(img, num_segments=100, num_iters=2):\n",
        "    \"\"\"\n",
        "    Generate non-overlapping CPMC-style object proposals for an image.\n",
        "    \"\"\"\n",
        "    scale_factor = 0.75  # Downscale factor\n",
        "    img_small = cv2.resize(img, (0, 0), fx=scale_factor, fy=scale_factor)\n",
        "    superpixels = generate_superpixels(img_small, num_segments=num_segments, compactness=30)\n",
        "\n",
        "    # Initialize global foreground mask (to track detected regions)\n",
        "    global_foreground_mask = np.zeros(img_small.shape[:2], dtype=np.uint8)\n",
        "\n",
        "    tasks = [\n",
        "        (img_small, (superpixels == label).astype(np.uint8), global_foreground_mask, iter_count)\n",
        "        for label in np.unique(superpixels)\n",
        "        for iter_count in range(1, num_iters + 1)\n",
        "    ]\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        proposals_small = list(executor.map(process_task, tasks))\n",
        "\n",
        "    # Resize proposals back to the original image size\n",
        "    proposals_resized = [cv2.resize(p, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "                         for p in proposals_small]\n",
        "\n",
        "    # Filter out small proposals\n",
        "    min_size = 500  # Minimum number of pixels for a proposal to be considered\n",
        "    filtered_proposals = [p for p in proposals_resized if np.sum(p) > min_size]\n",
        "\n",
        "    # Convert binary masks to colored proposals\n",
        "    colored_proposals = []\n",
        "    for mask in filtered_proposals:\n",
        "        mask_3 = np.repeat(mask[:, :, np.newaxis], 3, axis=2)  # Convert to 3-channel mask\n",
        "        colored = img * mask_3\n",
        "        colored_proposals.append(colored)\n",
        "\n",
        "    return colored_proposals\n",
        "\n",
        "def process_video_frames(video_path, num_segments=100, num_iters=2, max_frames=None, target_fps=15, save_dir=\"output_proposals\"):\n",
        "    \"\"\"\n",
        "    Process a video file frame-by-frame and generate non-overlapping object proposals.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(\"Error opening video file\")\n",
        "\n",
        "    orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    skip_frames = int(orig_fps / target_fps)\n",
        "    print(f\"Original FPS: {orig_fps}, processing every {skip_frames}th frame to get ~{target_fps} fps.\")\n",
        "\n",
        "    frame_proposals = {}\n",
        "    frame_count = 0\n",
        "    processed_frames = 0\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame_count += 1\n",
        "\n",
        "        if frame_count % skip_frames != 0:\n",
        "            continue\n",
        "\n",
        "        processed_frames += 1\n",
        "        print(f\"Processing frame {frame_count} (processed {processed_frames} frames)...\")\n",
        "        proposals = cpmc_object_proposals(frame, num_segments=num_segments, num_iters=num_iters)\n",
        "\n",
        "        # Filter proposals based on area\n",
        "        h, w = frame.shape[:2]\n",
        "        filtered = [p for p in proposals if 100 < np.count_nonzero(p) < 0.8 * h * w]\n",
        "        frame_proposals[frame_count] = filtered\n",
        "\n",
        "        # Save proposals\n",
        "        frame_dir = os.path.join(save_dir, f\"frame_{frame_count}\")\n",
        "        os.makedirs(frame_dir, exist_ok=True)\n",
        "        for i, prop in enumerate(filtered):\n",
        "            proposal_path = os.path.join(frame_dir, f\"proposal_{i}.png\")\n",
        "            cv2.imwrite(proposal_path, prop)\n",
        "\n",
        "        if max_frames is not None and processed_frames >= max_frames:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    return frame_proposals\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    video_path = \"/content/drive/MyDrive/data_composite_01/data/s0001_f_w000006.mp4\"\n",
        "    proposals_dict = process_video_frames(video_path, num_segments=10, num_iters=1, max_frames=None, target_fps=15)\n",
        "\n",
        "    # Print number of proposals per frame\n",
        "    for frame_idx, proposals in proposals_dict.items():\n",
        "        print(f\"Frame {frame_idx}: {len(proposals)} proposals\")\n"
      ],
      "metadata": {
        "id": "duDQ1NHf3QX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SCGP ALGO"
      ],
      "metadata": {
        "id": "k-p3QlSq3V3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import concurrent.futures\n",
        "\n",
        "##############################################\n",
        "# SCGP FUNCTIONS FOR VIDEO-WIDE VISUAL ATOM EXTRACTION\n",
        "##############################################\n",
        "\n",
        "# --- Step 1: Load Pretrained AlexNet Model for Feature Extraction ---\n",
        "class AlexNetFc7(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNetFc7, self).__init__()\n",
        "        alexnet = models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1)\n",
        "        self.features = alexnet.features\n",
        "        self.avgpool = alexnet.avgpool\n",
        "        self.fc6 = alexnet.classifier[1]\n",
        "        self.relu6 = alexnet.classifier[2]\n",
        "        self.fc7 = alexnet.classifier[4]\n",
        "        self.relu7 = alexnet.classifier[5]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc6(x)\n",
        "        x = self.relu6(x)\n",
        "        x = self.fc7(x)\n",
        "        x = self.relu7(x)\n",
        "        return x\n",
        "\n",
        "# Define image transforms for AlexNet.\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to AlexNet's expected input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --- Utility: Compute Bounding Box from a Colored Proposal ---\n",
        "def colored_proposal_to_bbox(colored_prop, thresh=1):\n",
        "    \"\"\"\n",
        "    Given a colored object proposal (an RGB image with a black background),\n",
        "    convert it to grayscale and compute a bounding box from non-black pixels.\n",
        "\n",
        "    Arguments:\n",
        "        colored_prop: The colored proposal image (as a NumPy array, BGR order).\n",
        "        thresh: Threshold value; pixels with intensity greater than this (in grayscale)\n",
        "                are considered foreground.\n",
        "\n",
        "    Returns:\n",
        "        A list [x_min, y_min, x_max, y_max] representing the bounding box, or None if no foreground is found.\n",
        "    \"\"\"\n",
        "    # Convert the colored proposal (BGR) to grayscale.\n",
        "    gray = cv2.cvtColor(colored_prop, cv2.COLOR_BGR2GRAY)\n",
        "    # Threshold the grayscale image: pixels > thresh are foreground.\n",
        "    _, binary = cv2.threshold(gray, thresh, 255, cv2.THRESH_BINARY)\n",
        "    ys, xs = np.where(binary > 0)\n",
        "    if ys.size == 0 or xs.size == 0:\n",
        "        return None\n",
        "    return [int(xs.min()), int(ys.min()), int(xs.max()), int(ys.max())]\n",
        "\n",
        "# --- Step 2: Extract Features from Object Proposals Using Bounding Boxes ---\n",
        "def extract_features_from_bounding_boxes(video_path, proposals_dict, device, model):\n",
        "    \"\"\"\n",
        "    For each colored object proposal in proposals_dict, compute its bounding box\n",
        "    (from non-black foreground), crop the corresponding region from the original frame,\n",
        "    and extract an fc7 feature vector from the cropped region.\n",
        "\n",
        "    Returns:\n",
        "        all_features: A numpy array of shape (total_proposals, feature_dim)\n",
        "        proposal_mapping: A list mapping each feature index to (frame_num, proposal_idx)\n",
        "    \"\"\"\n",
        "    all_features = []\n",
        "    proposal_mapping = []\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    for frame_num, proposals in proposals_dict.items():\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num - 1)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(f\"Could not read frame {frame_num}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        for idx, colored_prop in enumerate(proposals):\n",
        "            # Compute bounding box from the colored proposal.\n",
        "            bbox = colored_proposal_to_bbox(colored_prop)\n",
        "            if bbox is None:\n",
        "                continue\n",
        "            x_min, y_min, x_max, y_max = bbox\n",
        "            # Crop the object region from the original frame.\n",
        "            cropped_obj = frame[y_min:y_max, x_min:x_max]\n",
        "            if cropped_obj.size == 0:\n",
        "                continue\n",
        "\n",
        "            # Convert cropped region to PIL image (RGB) and apply transform.\n",
        "            pil_crop = Image.fromarray(cv2.cvtColor(cropped_obj, cv2.COLOR_BGR2RGB))\n",
        "            input_tensor = transform(pil_crop).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                feat = model(input_tensor)\n",
        "            all_features.append(feat.cpu().numpy().flatten())\n",
        "            proposal_mapping.append((frame_num, idx))\n",
        "    cap.release()\n",
        "\n",
        "    if all_features:\n",
        "        all_features = np.vstack(all_features)\n",
        "    else:\n",
        "        all_features = np.array([])\n",
        "    return all_features, proposal_mapping\n",
        "\n",
        "# --- Step 3: Compute Cosine Similarity Matrix ---\n",
        "# def cosine_similarity_matrix(features):\n",
        "#     \"\"\"\n",
        "#     Compute the cosine similarity matrix between feature vectors.\n",
        "#     \"\"\"\n",
        "#     norms = np.linalg.norm(features, axis=1, keepdims=True) + 1e-8\n",
        "#     features_norm = features / norms\n",
        "#     return features_norm @ features_norm.T\n",
        "def cosine_similarity_matrix(features):\n",
        "    \"\"\"\n",
        "    Compute the cosine similarity matrix between feature vectors.\n",
        "    \"\"\"\n",
        "    norms = np.linalg.norm(features, axis=1, keepdims=True) + 1e-8  # Avoid division by zero\n",
        "    features_norm = features / norms\n",
        "    similarity_matrix = features_norm @ features_norm.T\n",
        "\n",
        "    # Explicitly set diagonal to 1 to correct numerical precision errors\n",
        "    np.fill_diagonal(similarity_matrix, 1.0)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "# --- Step 4: Find Optimal Threshold Incrementally ---\n",
        "def find_optimal_threshold_rank1(u):\n",
        "    \"\"\"\n",
        "    Rank-1 thresholding to find subset v that maximizes (sum of top-n elements)^2 / n.\n",
        "    Returns the threshold value, best score, and the number of elements.\n",
        "    \"\"\"\n",
        "    u_min, u_max = u.min(), u.max()\n",
        "    if u_max - u_min < 1e-12:\n",
        "        return u[0], 0.0, len(u)\n",
        "    u_norm = (u - u_min) / (u_max - u_min)\n",
        "\n",
        "    sorted_idx = np.argsort(-u_norm)\n",
        "    u_sorted = u_norm[sorted_idx]\n",
        "\n",
        "    best_r = -np.inf\n",
        "    best_t = None\n",
        "    best_n = 0\n",
        "    running_sum = 0.0\n",
        "\n",
        "    for n in range(1, len(u_sorted) + 1):\n",
        "        running_sum += u_sorted[n-1]\n",
        "        score_n = (running_sum ** 2) / n\n",
        "        if score_n > best_r:\n",
        "            best_r = score_n\n",
        "            best_n = n\n",
        "            best_t = u_sorted[n-1]\n",
        "    return best_t, best_r, best_n\n",
        "\n",
        "# --- Step 5: Perform SCGP Clustering ---\n",
        "def extract_video_wide_clusters(features, min_cluster_size=3):\n",
        "    \"\"\"\n",
        "    Iteratively extracts visual atoms (clusters) from feature vectors using an optimal threshold.\n",
        "    Returns a list of clusters, where each cluster is an array of global indices (into features).\n",
        "    \"\"\"\n",
        "    if features.shape[0] == 0:\n",
        "        print(\"No valid proposals for clustering.\")\n",
        "        return []\n",
        "\n",
        "    clusters = []\n",
        "    remaining_indices = np.arange(features.shape[0])\n",
        "    features_remaining = features.copy()\n",
        "\n",
        "    iteration = 0\n",
        "    while features_remaining.shape[0] > 0:\n",
        "        iteration += 1\n",
        "\n",
        "        A = cosine_similarity_matrix(features_remaining)\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(A)\n",
        "        idx = np.argmax(eigenvalues)\n",
        "        dominant_vec = np.real(eigenvectors[:, idx])\n",
        "\n",
        "        u = (dominant_vec - dominant_vec.min()) / (dominant_vec.max() - dominant_vec.min() + 1e-8)\n",
        "\n",
        "        t_opt, best_r, best_n = find_optimal_threshold_rank1(u)\n",
        "\n",
        "        cluster_indicator = u >= t_opt\n",
        "        selected_idx = np.where(cluster_indicator)[0]\n",
        "        cluster_indices = remaining_indices[selected_idx]\n",
        "\n",
        "        if cluster_indices.size < min_cluster_size:\n",
        "            print(f\"Stopping at iteration {iteration}: Remaining proposals too few for meaningful clusters.\")\n",
        "            break\n",
        "\n",
        "        print(f\"Visual Atom {iteration}: Contains {len(cluster_indices)} proposals (threshold = {t_opt:.3f}, best_r = {best_r:.3f}).\")\n",
        "        clusters.append(cluster_indices)\n",
        "\n",
        "        mask = np.ones(features_remaining.shape[0], dtype=bool)\n",
        "        mask[selected_idx] = False\n",
        "        features_remaining = features_remaining[mask]\n",
        "        remaining_indices = remaining_indices[mask]\n",
        "\n",
        "    return clusters\n",
        "\n",
        "##############################################\n",
        "# MAIN SCGP CODE FOR VIDEO-WIDE CLUSTERING\n",
        "##############################################\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        proposals_dict  # proposals_dict should be generated by your CPMC code.\n",
        "    except NameError:\n",
        "        print(\"Proposals dictionary not found. Please generate proposals using the CPMC code first.\")\n",
        "        exit(1)\n",
        "\n",
        "    video_path = \"/content/drive/MyDrive/data_composite_01/data/s0001_f_w000006.mp4\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = AlexNetFc7().to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(\"\\nExtracting features for all proposals (using bounding boxes) across frames...\")\n",
        "    features, proposal_mapping = extract_features_from_bounding_boxes(video_path, proposals_dict, device, model)\n",
        "    print(\"Total proposals extracted:\", len(proposal_mapping))\n",
        "\n",
        "    print(\"\\nPerforming SCGP clustering across all frames...\")\n",
        "    clusters = extract_video_wide_clusters(features, min_cluster_size=10)\n",
        "    print(\"Total visual atoms (clusters) formed:\", len(clusters))\n",
        "\n",
        "    # Optional: Visualize clustering results (e.g., show frames for each cluster)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    clustered_frames = {}\n",
        "    for cluster in clusters:\n",
        "        for idx in cluster:\n",
        "            frame_num, proposal_idx = proposal_mapping[idx]\n",
        "            if frame_num not in clustered_frames:\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num - 1)\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    continue\n",
        "                clustered_frames[frame_num] = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    cap.release()\n",
        "\n",
        "    # (Visualization code can be extended as needed.)\n"
      ],
      "metadata": {
        "id": "ZgesNYeY3X1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INFERENCE"
      ],
      "metadata": {
        "id": "7Ykpifzc3bcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Visual Atoms (Clusters) and their corresponding proposal indices:\")\n",
        "\n",
        "for atom_num, cluster_indices in enumerate(clusters[:10], start=1):  # Limit to first 10 visual atoms\n",
        "    if isinstance(cluster_indices, np.ndarray):\n",
        "        cluster_indices = cluster_indices.tolist()  # Convert NumPy array to list\n",
        "\n",
        "    proposals_in_atom = [proposal_mapping[i] for i in cluster_indices]\n",
        "    print(f\"Visual Atom {atom_num}: Contains proposals with indices (frame_num, proposal_idx): {proposals_in_atom}\")\n"
      ],
      "metadata": {
        "id": "qg_D6yz83dRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# ----- Configuration -----\n",
        "# Parent directory to save visual atom proposals.\n",
        "parent_dir = \"visual_atoms_masks\"\n",
        "os.makedirs(parent_dir, exist_ok=True)\n",
        "\n",
        "# ----- Save Visual Atom Proposals -----\n",
        "# Iterate over each visual atom (cluster)\n",
        "for atom_num, cluster in enumerate(clusters, start=1):\n",
        "    # Create a folder for the current visual atom.\n",
        "    atom_folder = os.path.join(parent_dir, f\"visual_atom_{atom_num}\")\n",
        "    os.makedirs(atom_folder, exist_ok=True)\n",
        "\n",
        "    # Iterate over each proposal index in the cluster.\n",
        "    for idx in cluster:\n",
        "        # Retrieve the corresponding (frame_num, proposal_idx) from proposal_mapping.\n",
        "        frame_num, proposal_idx = proposal_mapping[idx]\n",
        "\n",
        "        # Retrieve the colored proposal image from proposals_dict.\n",
        "        # It is assumed that proposals_dict is a dictionary where each key is a frame number\n",
        "        # and each value is a list of colored proposals (RGB images).\n",
        "        proposal = proposals_dict[frame_num][proposal_idx]\n",
        "\n",
        "        # If the proposal image is stored as a float image in [0, 1], convert it to 0-255.\n",
        "        if proposal.dtype in [np.float32, np.float64]:\n",
        "            proposal = np.clip(proposal * 255, 0, 255).astype(np.uint8)\n",
        "        else:\n",
        "            proposal = proposal.astype(np.uint8)\n",
        "\n",
        "\n",
        "\n",
        "        # Create a filename and save the colored proposal.\n",
        "        filename = f\"frame_{frame_num}_proposal_{proposal_idx}.png\"\n",
        "        file_path = os.path.join(atom_folder, filename)\n",
        "        cv2.imwrite(file_path, proposal)\n",
        "\n",
        "print(\"Visual atom proposals have been saved in the folder 'visual_atoms_masks'.\")\n"
      ],
      "metadata": {
        "id": "A5Jpqb303hWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the folder\n",
        "!zip -r /content/visual_atoms_masks.zip /content/visual_atoms_masks\n",
        "\n",
        "# Download the zipped folder\n",
        "from google.colab import files\n",
        "files.download(\"/content/visual_atoms_masks.zip\")\n"
      ],
      "metadata": {
        "id": "mTqgzl9W3i9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "img = cv2.imread('frame_28_proposal_9.png')\n",
        "g = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "_, t = cv2.threshold(g, 1, 255, cv2.THRESH_BINARY)\n",
        "c = cv2.findNonZero(t)\n",
        "x, y, w, h = cv2.boundingRect(c)\n",
        "crop = img[y:y+h, x:x+w]\n",
        "resized = cv2.resize(crop, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
        "cv2_imshow(resized)\n"
      ],
      "metadata": {
        "id": "4DFaq3Hv3mBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume these variables are already available from your SCGP pipeline:\n",
        "# - features: a NumPy array of shape (total_proposals, feature_dim)\n",
        "# - clusters: a list of clusters where each cluster is an array of indices (into features)\n",
        "# - cosine_similarity_matrix: your function that computes the cosine similarity matrix\n",
        "\n",
        "# For Visual Atom 1 (first cluster):\n",
        "if len(clusters) == 0:\n",
        "    print(\"No clusters were found.\")\n",
        "else:\n",
        "    cluster2_indices = clusters[1]  # Get indices for the first visual atom\n",
        "    print(\"Visual Atom 2 contains proposals with indices:\", cluster2_indices)\n",
        "\n",
        "    # Extract features corresponding to these proposals\n",
        "    features_cluster2 = features[cluster2_indices]\n",
        "\n",
        "    # Compute the cosine similarity matrix for proposals in Visual Atom 1\n",
        "    similarity_matrix = cosine_similarity_matrix(features_cluster2)\n",
        "\n",
        "    # Print the similarity matrix\n",
        "    print(\"Cosine similarity matrix for Visual Atom 2:\")\n",
        "    print(similarity_matrix)\n"
      ],
      "metadata": {
        "id": "JQBdxKAi3l-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose target is the tuple representing (frame_number, proposal_index)\n",
        "target = (28, 7)\n",
        "\n",
        "# Loop over proposal_mapping to find the global index\n",
        "global_index = None\n",
        "for i, mapping in enumerate(proposal_mapping):\n",
        "    if mapping == target:\n",
        "        global_index = i\n",
        "        break\n",
        "\n",
        "if global_index is not None:\n",
        "    print(f\"Global index for {target} is {global_index}.\")\n",
        "else:\n",
        "    print(f\"{target} not found in the proposal mapping.\")\n"
      ],
      "metadata": {
        "id": "CpwhsOhh3l71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(clusters) < 2:\n",
        "    print(\"Visual Atom 2 does not exist.\")\n",
        "else:\n",
        "    cluster2_indices = clusters[1]  # Get indices for Visual Atom 2\n",
        "    print(\"Visual Atom 2 contains proposals with indices:\", cluster2_indices)\n",
        "\n",
        "    if len(cluster2_indices) < 2:\n",
        "        print(\"Not enough proposals in Visual Atom 2 to compute pairwise similarity.\")\n",
        "    else:\n",
        "        # Extract features corresponding to these proposals\n",
        "        features_cluster2 = features[cluster2_indices]\n",
        "\n",
        "        # Compute the cosine similarity matrix for Visual Atom 2\n",
        "        similarity_matrix = cosine_similarity_matrix(features_cluster2)\n",
        "\n",
        "        # Print the similarity matrix\n",
        "        print(\"Cosine similarity matrix for Visual Atom 2:\")\n",
        "        print(similarity_matrix)\n"
      ],
      "metadata": {
        "id": "k87kF2br3l49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- Step 1: Identify the target proposal's index within Visual Atom 2 ----\n",
        "target = (28, 7)  # Example: Target is the 8th proposal of frame 28\n",
        "\n",
        "# Get global indices of proposals in Visual Atom 2\n",
        "cluster2_indices = np.array(clusters[1])  # Visual Atom 2\n",
        "cluster2_mapping = [proposal_mapping[i] for i in cluster2_indices]  # Map to (frame, proposal)\n",
        "\n",
        "# Find the local index of the target proposal **within Visual Atom 2**\n",
        "if target in cluster2_mapping:\n",
        "    target_local_idx = cluster2_mapping.index(target)  # Local index within Visual Atom 2\n",
        "    target_global_idx = cluster2_indices[target_local_idx]  # Global index in full dataset\n",
        "else:\n",
        "    print(f\"Target proposal {target} not found in Visual Atom 2.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Target proposal {target} is at global index {target_global_idx} (local index {target_local_idx}) within Visual Atom 2.\")\n",
        "\n",
        "# ---- Step 2: Extract similarity values **only for Visual Atom 2** ----\n",
        "# Map the similarity matrix to use **only local indices of Visual Atom 2**\n",
        "# Call the function to compute the cosine similarity matrix using your feature data\n",
        "cosine_sim_matrix = cosine_similarity_matrix(features)  # Pass the correct feature data\n",
        "sim_matrix_atom2 = cosine_sim_matrix[np.ix_(cluster2_indices, cluster2_indices)]\n",
        "\n",
        "# Get the similarity row corresponding to the target proposal (within Visual Atom 2)\n",
        "sim_row_within_atom2 = sim_matrix_atom2[target_local_idx].copy()\n",
        "sim_row_within_atom2[target_local_idx] = -np.inf  # Exclude self-similarity\n",
        "\n",
        "# ---- Step 3: Find the top 5 highest similarity proposals **within Visual Atom 2** ----\n",
        "top5_local_indices = np.argsort(sim_row_within_atom2)[-5:][::-1]  # Sorted descending\n",
        "\n",
        "print(\"\\nTop 5 similar proposals (within Visual Atom 2, global indices):\")\n",
        "top5_global_indices = cluster2_indices[top5_local_indices]  # Convert back to global indices\n",
        "\n",
        "for idx in top5_global_indices:\n",
        "    frame_num, proposal_idx = proposal_mapping[idx]\n",
        "    similarity_value = cosine_sim_matrix[target_global_idx, idx]\n",
        "    print(f\"Global index {idx}: Frame {frame_num}, Proposal {proposal_idx}, Similarity = {similarity_value:.4f}\")\n",
        "\n",
        "    # ---- Step 4: Display the proposals ----\n",
        "    if frame_num in proposals_dict and proposal_idx < len(proposals_dict[frame_num]):\n",
        "        prop_img = proposals_dict[frame_num][proposal_idx]\n",
        "        plt.figure(figsize=(4,4))\n",
        "        plt.imshow(cv2.cvtColor(prop_img, cv2.COLOR_BGR2RGB))  # Convert from BGR to RGB\n",
        "        plt.title(f\"Frame {frame_num} - Proposal {proposal_idx} (Similarity: {similarity_value:.4f})\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Proposal for frame {frame_num}, proposal index {proposal_idx} not found.\")\n"
      ],
      "metadata": {
        "id": "N4r5wNT_3l2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#USING MEDIAPIPE,AUTOENCODER,KMEANS CLUSTERING AND HMM"
      ],
      "metadata": {
        "id": "MSeprgkJ39qN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MEDIAPIPE KEYPOINT EXTRACTION AND KEYPOINTS NORMALIZATION"
      ],
      "metadata": {
        "id": "ltlCL2Q94J4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe\n",
        "!pip install opencv-python\n",
        "!pip install numpy==1.26.4\n",
        "!pip install --upgrade pandas mediapipe"
      ],
      "metadata": {
        "id": "stG9S1kA3lza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp"
      ],
      "metadata": {
        "id": "3wsB1kXv3lwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "# import mediapipe as mp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "import os"
      ],
      "metadata": {
        "id": "rroZM3aB3ltj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pickle\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "\n",
        "# Input and Output Paths\n",
        "input_video_path = '/content/drive/MyDrive/data_composite_01/data/s0001_f_w000067.mp4'\n",
        "output_pickle_path = '/content/drive/MyDrive/s0001_f_w000067.pkl'\n",
        "\n",
        "# Initialize MediaPipe Holistic\n",
        "mp_holistic = mp.solutions.holistic\n",
        "holistic = mp_holistic.Holistic(static_image_mode=False,\n",
        "                                model_complexity=2,\n",
        "                                enable_segmentation=False,\n",
        "                                refine_face_landmarks=False)\n",
        "\n",
        "# Open video\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "\n",
        "# Storage for keypoints\n",
        "left_hand_all = []\n",
        "right_hand_all = []\n",
        "pose_all = []\n",
        "\n",
        "# Process each frame\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert the frame to RGB\n",
        "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = holistic.process(image_rgb)\n",
        "\n",
        "    # Get left hand keypoints (21 x 3)\n",
        "    if results.left_hand_landmarks:\n",
        "        left_hand = np.array([[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark])\n",
        "    else:\n",
        "        left_hand = np.zeros((21, 3))\n",
        "    left_hand_all.append(left_hand)\n",
        "\n",
        "    # Get right hand keypoints (21 x 3)\n",
        "    if results.right_hand_landmarks:\n",
        "        right_hand = np.array([[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark])\n",
        "    else:\n",
        "        right_hand = np.zeros((21, 3))\n",
        "    right_hand_all.append(right_hand)\n",
        "\n",
        "    # Get pose keypoints (33 x 4) – including visibility\n",
        "    if results.pose_landmarks:\n",
        "        pose = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in results.pose_landmarks.landmark])\n",
        "    else:\n",
        "        pose = np.zeros((33, 4))\n",
        "    pose_all.append(pose)\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "holistic.close()\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "left_hand_all = np.stack(left_hand_all)\n",
        "right_hand_all = np.stack(right_hand_all)\n",
        "pose_all = np.stack(pose_all)\n",
        "\n",
        "# Create dictionary\n",
        "data_dict = {\n",
        "    \"Video height\": video_height,\n",
        "    \"Video width\": video_width,\n",
        "    \"left hand\": left_hand_all,\n",
        "    \"right hand\": right_hand_all,\n",
        "    \"pose\": pose_all\n",
        "}\n",
        "\n",
        "# Save to pickle\n",
        "with open(output_pickle_path, 'wb') as f:\n",
        "    pickle.dump(data_dict, f)\n",
        "\n",
        "print(\"Keypoints successfully saved to:\", output_pickle_path)\n"
      ],
      "metadata": {
        "id": "MeeO2_AM3lq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pose connection map (based on your diagram)\n",
        "POSE_CONNECTIONS = [\n",
        "    (0, 1), (1, 2), (2, 3), (3, 7),  # right eye\n",
        "    (0, 4), (4, 5), (5, 6), (6, 8),  # left eye\n",
        "    (9, 10),                         # mouth\n",
        "    (11, 12), (11, 13), (13, 15),    # right arm\n",
        "    (12, 14), (14, 16),              # left arm\n",
        "    (11, 23), (12, 24),              # torso\n",
        "    (23, 24), (23, 25), (24, 26),    # hips\n",
        "    (25, 27), (27, 29), (29, 31),    # right leg\n",
        "    (26, 28), (28, 30), (30, 32)     # left leg\n",
        "]\n",
        "\n",
        "HAND_CONNECTIONS = [\n",
        "    (0, 1), (1, 2), (2, 3), (3, 4),      # thumb\n",
        "    (0, 5), (5, 6), (6, 7), (7, 8),      # index\n",
        "    (0, 9), (9, 10), (10, 11), (11, 12), # middle\n",
        "    (0, 13), (13, 14), (14, 15), (15, 16), # ring\n",
        "    (0, 17), (17, 18), (18, 19), (19, 20)  # pinky\n",
        "]\n",
        "\n",
        "class SkeletonVisualizer:\n",
        "    def __init__(self, pickle_path):\n",
        "        with open(pickle_path, 'rb') as f:\n",
        "            self.data = pickle.load(f)\n",
        "        self.video_height = self.data[\"Video height\"]\n",
        "        self.video_width = self.data[\"Video width\"]\n",
        "        self.left_hand = self.data[\"left hand\"]\n",
        "        self.right_hand = self.data[\"right hand\"]\n",
        "        self.pose = self.data[\"pose\"]\n",
        "\n",
        "    def __getitem__(self, frame_idx):\n",
        "        lh = self.left_hand[frame_idx] * [self.video_width, self.video_height, 1]\n",
        "        rh = self.right_hand[frame_idx] * [self.video_width, self.video_height, 1]\n",
        "        pose = self.pose[frame_idx][:, :3] * [self.video_width, self.video_height, 1]\n",
        "        self.plot_skeleton(pose, lh[:, :3], rh[:, :3], frame_idx)\n",
        "        print(f\"\\nFrame {frame_idx}\")\n",
        "        print(\"Left Hand:\\n\", lh)\n",
        "        print(\"Right Hand:\\n\", rh)\n",
        "        print(\"Pose:\\n\", pose)\n",
        "\n",
        "    def plot_skeleton(self, pose, lh, rh, frame_idx):\n",
        "        print(lh)\n",
        "        print(rh)\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        ax = plt.gca()\n",
        "        ax.set_title(f\"Skeleton Frame {frame_idx}\")\n",
        "        ax.set_xlim(0, self.video_width)\n",
        "        ax.set_ylim(self.video_height, 0)  # Flip y-axis\n",
        "\n",
        "        # Pose connections\n",
        "        for i, j in POSE_CONNECTIONS:\n",
        "            plt.plot([pose[i, 0], pose[j, 0]], [pose[i, 1], pose[j, 1]], 'g-', lw=2)\n",
        "        plt.scatter(pose[:, 0], pose[:, 1], c='r', s=10, label='Pose')\n",
        "\n",
        "        # Left hand\n",
        "        lh_wrist = pose[15, 2]  # Left wrist as base\n",
        "        lh_coords = lh[:, :2] + lh_wrist  # Offset relative to wrist\n",
        "        for i, j in HAND_CONNECTIONS:\n",
        "            plt.plot([lh_coords[i, 0], lh_coords[j, 0]], [lh_coords[i, 1], lh_coords[j, 1]], 'b-', lw=1.5)\n",
        "        plt.scatter(lh_coords[:, 0], lh_coords[:, 1], c='blue', s=8, label='Left Hand')\n",
        "\n",
        "        # Right hand\n",
        "        rh_coords = rh[:, :2]  # Offset relative to wrist\n",
        "        print(rh_coords)\n",
        "        for i, j in HAND_CONNECTIONS:\n",
        "            plt.plot([rh_coords[i, 0], rh_coords[j, 0]], [rh_coords[i, 1], rh_coords[j, 1]], 'm-', lw=1.5)\n",
        "        plt.scatter(rh_coords[:, 0], rh_coords[:, 1], c='magenta', s=8, label='Right Hand')\n",
        "\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "g-cT9Yfh3loT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vis = SkeletonVisualizer(\"/content/drive/MyDrive/s0001_f_w000067.pkl\")\n",
        "vis[75]  # Show frame 50\n"
      ],
      "metadata": {
        "id": "luWcCNw33llh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class SkeletonVisualizer:\n",
        "    def __init__(self, pickle_path):\n",
        "        with open(pickle_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.video_height = data['Video height']\n",
        "        self.video_width = data['Video width']\n",
        "        self.left_hand = data['left hand']\n",
        "        self.right_hand = data['right hand']\n",
        "        self.pose = data['pose']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pose)\n",
        "\n",
        "    def __getitem__(self, frame_idx):\n",
        "        lh = self.left_hand[frame_idx] * [self.video_width, self.video_height, 1]\n",
        "        rh = self.right_hand[frame_idx] * [self.video_width, self.video_height, 1]\n",
        "        pose = self.pose[frame_idx][:, :3] * [self.video_width, self.video_height, 1]\n",
        "\n",
        "        self.plot_pose(pose, frame_idx)\n",
        "        self.plot_hand(lh[:, :2], 'Left Hand', 'blue', frame_idx)\n",
        "        self.plot_hand(rh[:, :2], 'Right Hand', 'magenta', frame_idx)\n",
        "\n",
        "    def plot_pose(self, pose_xyc, frame_idx):\n",
        "        POSE_CONNECTIONS = [\n",
        "            (0, 1), (1, 2), (2, 3), (3, 7),\n",
        "            (0, 4), (4, 5), (5, 6), (6, 8),\n",
        "            (9, 10), (11, 12), (11, 13), (13, 15),\n",
        "            (12, 14), (14, 16), (15, 17),(15,19),(15,21), (16, 18),(16,20),(16,22),\n",
        "            (11, 23), (12, 24), (23, 24), (23, 25),\n",
        "            (24, 26), (25, 27), (26, 28),\n",
        "            (27, 29), (28, 30), (29, 31), (30, 32)\n",
        "        ]\n",
        "\n",
        "        x, y = pose_xyc[:, 0], pose_xyc[:, 1]\n",
        "\n",
        "        plt.figure(figsize=(6, 8))\n",
        "        plt.scatter(x, y, c='red', label='Pose')\n",
        "        for connection in POSE_CONNECTIONS:\n",
        "            x0, y0 = pose_xyc[connection[0], 0], pose_xyc[connection[0], 1]\n",
        "            x1, y1 = pose_xyc[connection[1], 0], pose_xyc[connection[1], 1]\n",
        "            plt.plot([x0, x1], [y0, y1], color='green')\n",
        "\n",
        "        plt.title(f'Pose Skeleton Frame {frame_idx}')\n",
        "        plt.xlim(0, self.video_width)\n",
        "        plt.ylim(0, self.video_height)\n",
        "        plt.gca().invert_yaxis()  # Fix upside-down issue\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_hand(self, hand_coords, hand_name, color, frame_idx):\n",
        "        HAND_CONNECTIONS = [\n",
        "            (0,1),(0,17),(1,2),(2,3),(3,4),\n",
        "            (0,5),(5,6),(6,7),(7,8),\n",
        "            (5,9),(9,10),(10,11),(11,12),\n",
        "            (9,13),(13,14),(14,15),(15,16),\n",
        "            (13,17),(17,18),(18,19),(19,20)\n",
        "        ]\n",
        "\n",
        "        x, y = hand_coords[:, 0], hand_coords[:, 1]\n",
        "\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.scatter(x, y, c=color, label=hand_name)\n",
        "        for connection in HAND_CONNECTIONS:\n",
        "            x0, y0 = hand_coords[connection[0], 0], hand_coords[connection[0], 1]\n",
        "            x1, y1 = hand_coords[connection[1], 0], hand_coords[connection[1], 1]\n",
        "            plt.plot([x0, x1], [y0, y1], color=color)\n",
        "\n",
        "        # Zoom around hand keypoints\n",
        "        margin = 30\n",
        "        x_min, x_max = np.min(x) - margin, np.max(x) + margin\n",
        "        y_min, y_max = np.min(y) - margin, np.max(y) + margin\n",
        "        plt.xlim(x_min, x_max)\n",
        "        plt.ylim(y_max, y_min)  # Inverted manually for zoom view\n",
        "\n",
        "        plt.title(f'{hand_name} Skeleton Frame {frame_idx}')\n",
        "        #plt.gca().invert_yaxis()  # Fix upside-down issue\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "PxVGwgTL3ljC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vis = SkeletonVisualizer('/content/drive/MyDrive/s0001_f_w000067.pkl')\n",
        "vis[75]  # Change the number to visualize other frames\n"
      ],
      "metadata": {
        "id": "Bx0HPdQO3lgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkeletonVisualizer:\n",
        "    def __init__(self, pickle_path):\n",
        "        with open(pickle_path, 'rb') as f:\n",
        "            self.data = pickle.load(f)\n",
        "        self.video_height = self.data[\"Video height\"]\n",
        "        self.video_width = self.data[\"Video width\"]\n",
        "        self.left_hand = self.data[\"left hand\"]\n",
        "        self.right_hand = self.data[\"right hand\"]\n",
        "        self.pose = self.data[\"pose\"]\n",
        "\n",
        "    def __getitem__(self, frame_idx):\n",
        "        # Scale to pixel space\n",
        "        lh = self.left_hand[frame_idx] * [self.video_width, self.video_height, 1]\n",
        "        rh = self.right_hand[frame_idx] * [self.video_width, self.video_height, 1]\n",
        "        pose = self.pose[frame_idx][:, :3] * [self.video_width, self.video_height, 1]\n",
        "\n",
        "        self.plot_skeleton(pose, lh, rh, frame_idx)\n",
        "\n",
        "    def plot_skeleton(self, pose, lh, rh, frame_idx):\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        ax = plt.gca()\n",
        "        ax.set_title(f\"Skeleton Frame {frame_idx}\")\n",
        "        ax.set_xlim(0, self.video_width)\n",
        "        ax.set_ylim(self.video_height, 0)  # Flip y-axis\n",
        "\n",
        "        # Pose\n",
        "        for i, j in POSE_CONNECTIONS:\n",
        "            plt.plot([pose[i, 0], pose[j, 0]], [pose[i, 1], pose[j, 1]], 'g-', lw=2)\n",
        "        plt.scatter(pose[:, 0], pose[:, 1], c='r', s=10, label='Pose')\n",
        "\n",
        "        # Left hand (translated to left wrist)\n",
        "        lh_wrist_pose = pose[15, :2]\n",
        "        if not np.allclose(lh, 0):  # skip if all zeros\n",
        "            lh_relative = lh[:, :2] - lh[0, :2]  # shift hand relative to wrist center\n",
        "            lh_coords = lh_relative + lh_wrist_pose\n",
        "            for i, j in HAND_CONNECTIONS:\n",
        "                plt.plot([lh_coords[i, 0], lh_coords[j, 0]], [lh_coords[i, 1], lh_coords[j, 1]], 'b-', lw=1.5)\n",
        "            plt.scatter(lh_coords[:, 0], lh_coords[:, 1], c='blue', s=8, label='Left Hand')\n",
        "\n",
        "        # Right hand (translated to right wrist)\n",
        "        rh_wrist_pose = pose[16, :2]\n",
        "        if not np.allclose(rh, 0):  # skip if all zeros\n",
        "            rh_relative = rh[:, :2] - rh[0, :2]  # shift hand relative to wrist center\n",
        "            rh_coords = rh_relative + rh_wrist_pose\n",
        "            for i, j in HAND_CONNECTIONS:\n",
        "                plt.plot([rh_coords[i, 0], rh_coords[j, 0]], [rh_coords[i, 1], rh_coords[j, 1]], 'm-', lw=1.5)\n",
        "            plt.scatter(rh_coords[:, 0], rh_coords[:, 1], c='magenta', s=8, label='Right Hand')\n",
        "\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "98GLI9pJ3ldi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the visualizer with the path to your pickle file\n",
        "visualizer = SkeletonVisualizer('/content/drive/MyDrive/s0001_f_w000067.pkl')\n",
        "\n",
        "# Visualize the skeleton for frame 10\n",
        "visualizer[95]\n"
      ],
      "metadata": {
        "id": "qsGk0fOe4l7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "class SkeletonVisualizer3D:\n",
        "    def __init__(self, pose_connections, left_hand_connections, right_hand_connections, video_height=1080):\n",
        "        self.pose_connections = pose_connections\n",
        "        self.left_hand_connections = left_hand_connections\n",
        "        self.right_hand_connections = right_hand_connections\n",
        "        self.video_height = video_height\n",
        "\n",
        "    def plot_3d_skeleton(self, pose, lh=None, rh=None, frame_num=0):\n",
        "        fig = plt.figure(figsize=(10, 10))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.set_title(f\"3D Skeleton Frame {frame_num}\")\n",
        "\n",
        "        # Pose\n",
        "        if pose is not None:\n",
        "            x = pose[:, 0]\n",
        "            y = self.video_height - pose[:, 1]\n",
        "            z = -pose[:, 2]\n",
        "            ax.scatter(x, y, z, c='red', label='Pose')\n",
        "            for connection in self.pose_connections:\n",
        "                ax.plot(\n",
        "                    [x[connection[0]], x[connection[1]]],\n",
        "                    [y[connection[0]], y[connection[1]]],\n",
        "                    [z[connection[0]], z[connection[1]]],\n",
        "                    color='green'\n",
        "                )\n",
        "\n",
        "        # Left hand\n",
        "        if lh is not None:\n",
        "            x = lh[:, 0]\n",
        "            y = self.video_height - lh[:, 1]\n",
        "            z = -lh[:, 2]\n",
        "            ax.scatter(x, y, z, c='blue', label='Left Hand')\n",
        "            for connection in self.left_hand_connections:\n",
        "                ax.plot(\n",
        "                    [x[connection[0]], x[connection[1]]],\n",
        "                    [y[connection[0]], y[connection[1]]],\n",
        "                    [z[connection[0]], z[connection[1]]],\n",
        "                    color='blue'\n",
        "                )\n",
        "\n",
        "        # Right hand\n",
        "        if rh is not None:\n",
        "            x = rh[:, 0]\n",
        "            y = self.video_height - rh[:, 1]\n",
        "            z = -rh[:, 2]\n",
        "            ax.scatter(x, y, z, c='magenta', label='Right Hand')\n",
        "            for connection in self.right_hand_connections:\n",
        "                ax.plot(\n",
        "                    [x[connection[0]], x[connection[1]]],\n",
        "                    [y[connection[0]], y[connection[1]]],\n",
        "                    [z[connection[0]], z[connection[1]]],\n",
        "                    color='magenta'\n",
        "                )\n",
        "\n",
        "        ax.set_xlabel('X (Width)')\n",
        "        ax.set_ylabel('Y (Height)')\n",
        "        ax.set_zlabel('Z (Depth)')\n",
        "        ax.legend()\n",
        "        ax.view_init(elev=45, azim=-70)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "XJ-Qcu4P4l3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "class SkeletonVisualizer:\n",
        "    def __init__(self, pickle_path):\n",
        "        with open(pickle_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.video_height = data['Video height']\n",
        "        self.video_width = data['Video width']\n",
        "        self.left_hand = data['left hand']\n",
        "        self.right_hand = data['right hand']\n",
        "        self.pose = data['pose']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pose)\n",
        "\n",
        "\n",
        "    def __getitem__(self, frame_idx):\n",
        "          lh = self.left_hand[frame_idx] * [self.video_width, self.video_height, 1]\n",
        "          rh = self.right_hand[frame_idx] * [self.video_width, self.video_height, 1]\n",
        "          pose = self.pose[frame_idx][:, :3] * [self.video_width, self.video_height, 1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          # Adjust hand z-coordinates using corresponding pose wrist depth\n",
        "          lh[:, 2] += pose[15, 2]  # Left wrist\n",
        "          rh[:, 2] += pose[16, 2]  # Right wrist\n",
        "\n",
        "          # Reorder axes to make ZX the base and Y the vertical (flip Z to fix upside down)\n",
        "          lh = lh[:, [0, 2, 1]]\n",
        "          rh = rh[:, [0, 2, 1]]\n",
        "          pose = pose[:, [0, 2, 1]]\n",
        "\n",
        "          # Flip vertically to fix upside-down issue\n",
        "          lh[:, 2] = self.video_height - lh[:, 2]\n",
        "          rh[:, 2] = self.video_height - rh[:, 2]\n",
        "          pose[:, 2] = self.video_height - pose[:, 2]\n",
        "\n",
        "          self.plot_skeleton_3d(pose, lh, rh, frame_idx)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def plot_skeleton_3d(self, pose, left_hand, right_hand, frame_idx):\n",
        "        POSE_CONNECTIONS = [\n",
        "            (0, 1), (1, 2), (2, 3), (3, 7),\n",
        "            (0, 4), (4, 5), (5, 6), (6, 8),\n",
        "            (9, 10), (11, 12), (11, 13), (13, 15),\n",
        "            (12, 14), (14, 16), (15, 17),(15,19),(15,21),\n",
        "            (16, 18),(16,20),(16,22),\n",
        "            (11, 23), (12, 24), (23, 24), (23, 25),\n",
        "            (24, 26), (25, 27), (26, 28),\n",
        "            (27, 29), (28, 30), (29, 31), (30, 32)\n",
        "        ]\n",
        "\n",
        "        HAND_CONNECTIONS = [\n",
        "            (0,1),(0,17),(1,2),(2,3),(3,4),\n",
        "            (0,5),(5,6),(6,7),(7,8),\n",
        "            (5,9),(9,10),(10,11),(11,12),\n",
        "            (9,13),(13,14),(14,15),(15,16),\n",
        "            (13,17),(17,18),(18,19),(19,20)\n",
        "        ]\n",
        "\n",
        "        fig = plt.figure(figsize=(10, 10))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "        # Plot pose skeleton\n",
        "        for connection in POSE_CONNECTIONS:\n",
        "            p1, p2 = pose[connection[0]], pose[connection[1]]\n",
        "            ax.plot([p1[0], p2[0]], [p1[1], p2[1]], [p1[2], p2[2]], color='green')\n",
        "\n",
        "        ax.scatter(pose[:, 0], pose[:, 1], pose[:, 2], color='red', label='Pose')\n",
        "\n",
        "        # Plot left hand\n",
        "        for connection in HAND_CONNECTIONS:\n",
        "            p1, p2 = left_hand[connection[0]], left_hand[connection[1]]\n",
        "            ax.plot([p1[0], p2[0]], [p1[1], p2[1]], [p1[2], p2[2]], color='blue')\n",
        "        ax.scatter(left_hand[:, 0], left_hand[:, 1], left_hand[:, 2], color='blue', label='Left Hand')\n",
        "\n",
        "        # Plot right hand\n",
        "        for connection in HAND_CONNECTIONS:\n",
        "            p1, p2 = right_hand[connection[0]], right_hand[connection[1]]\n",
        "            ax.plot([p1[0], p2[0]], [p1[1], p2[1]], [p1[2], p2[2]], color='magenta')\n",
        "        ax.scatter(right_hand[:, 0], right_hand[:, 1], right_hand[:, 2], color='magenta', label='Right Hand')\n",
        "\n",
        "\n",
        "        ax.set_xlim(0, self.video_width)\n",
        "        ax.set_ylim(0, np.max([pose[:, 1].max(), left_hand[:, 1].max(), right_hand[:, 1].max()]) + 10)  # Depth\n",
        "        ax.set_zlim(0, self.video_height)  # Height\n",
        "        ax.set_xlabel('X')\n",
        "        ax.set_ylabel('Depth')\n",
        "        ax.set_zlabel('Height')\n",
        "\n",
        "\n",
        "        ax.view_init(elev=15, azim=-70)\n",
        "        ax.legend()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "MC5a6xG54l0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the visualizer with the path to your pickle file\n",
        "visualizer = SkeletonVisualizer('/content/drive/MyDrive/s0001_f_w000067.pkl')\n",
        "\n",
        "# Visualize the skeleton for frame 10\n",
        "visualizer[108]\n"
      ],
      "metadata": {
        "id": "ZWRIbtya4lxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def normalize_and_align_skeleton(pose, left_hand, right_hand):\n",
        "    \"\"\"\n",
        "    Apply scale and rotation normalization to 3D keypoints.\n",
        "    - Aligns middle finger tip to point upwards.\n",
        "    - Scales based on average bone length.\n",
        "    \"\"\"\n",
        "    def normalize_kpts(kpts):\n",
        "        if np.isnan(kpts).any():\n",
        "            return kpts  # skip if any NaNs\n",
        "        # Center around wrist for hands or mid-hip for pose\n",
        "        center = kpts[0]  # wrist or central joint\n",
        "        kpts_centered = kpts - center\n",
        "\n",
        "        # Normalize scale (bone length)\n",
        "        dists = np.linalg.norm(kpts_centered[1:] - center, axis=1)\n",
        "        scale = np.mean(dists) if np.mean(dists) > 0 else 1.0\n",
        "        kpts_scaled = kpts_centered / scale\n",
        "\n",
        "        # Align z-axis based on the middle finger or spine direction\n",
        "        # Use arbitrary vector (0, 1, 0) as target up\n",
        "        if len(kpts) == 21:\n",
        "            direction = kpts_scaled[12] - kpts_scaled[0]  # wrist to middle finger tip\n",
        "        elif len(kpts) == 33:\n",
        "            direction = kpts_scaled[11] - kpts_scaled[0]  # mid-spine to neck\n",
        "        else:\n",
        "            return kpts_scaled\n",
        "\n",
        "        direction /= np.linalg.norm(direction) + 1e-6\n",
        "        target = np.array([0, 1, 0])\n",
        "        v = np.cross(direction, target)\n",
        "        s = np.linalg.norm(v)\n",
        "        c = np.dot(direction, target)\n",
        "        if s == 0:\n",
        "            R = np.eye(3)\n",
        "        else:\n",
        "            vx = np.array([[0, -v[2], v[1]],\n",
        "                           [v[2], 0, -v[0]],\n",
        "                           [-v[1], v[0], 0]])\n",
        "            R = np.eye(3) + vx + (vx @ vx) * ((1 - c) / (s ** 2))\n",
        "        kpts_rotated = kpts_scaled @ R.T\n",
        "        return kpts_rotated\n",
        "\n",
        "    pose_new = normalize_kpts(pose[:, :3]) if pose is not None else None\n",
        "    left_new = normalize_kpts(left_hand) if left_hand is not None else None\n",
        "    right_new = normalize_kpts(right_hand) if right_hand is not None else None\n",
        "    return pose_new, left_new, right_new\n",
        "\n",
        "\n",
        "# Load original file\n",
        "input_path = '/content/drive/MyDrive/s0001_f_w000067.pkl'\n",
        "with open(input_path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "video_height = data['Video height']\n",
        "video_width = data['Video width']\n",
        "pose_data = data['pose']\n",
        "left_data = data['left hand']\n",
        "right_data = data['right hand']\n",
        "\n",
        "# Initialize new arrays\n",
        "transformed_pose = []\n",
        "transformed_left = []\n",
        "transformed_right = []\n",
        "\n",
        "for i in tqdm(range(len(pose_data))):\n",
        "    pose = pose_data[i]\n",
        "    left = left_data[i]\n",
        "    right = right_data[i]\n",
        "    p_t, l_t, r_t = normalize_and_align_skeleton(pose, left, right)\n",
        "    # Add back confidence score to pose\n",
        "    if p_t is not None:\n",
        "        conf = pose[:, 3:4]\n",
        "        p_t = np.concatenate([p_t, conf], axis=-1)\n",
        "    transformed_pose.append(p_t)\n",
        "    transformed_left.append(l_t)\n",
        "    transformed_right.append(r_t)\n",
        "\n",
        "transformed_data = {\n",
        "    'Video height': video_height,\n",
        "    'Video width': video_width,\n",
        "    'pose': np.array(transformed_pose),\n",
        "    'left hand': np.array(transformed_left),\n",
        "    'right hand': np.array(transformed_right),\n",
        "}\n",
        "\n",
        "# Save to a new pickle\n",
        "output_path = '/content/drive/MyDrive/s0001_f_w000067_transformed.pkl'\n",
        "with open(output_path, 'wb') as f:\n",
        "    pickle.dump(transformed_data, f)\n",
        "\n",
        "print(\"✅ Transformed keypoints saved to:\", output_path)\n"
      ],
      "metadata": {
        "id": "rsRoNk0O4luz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Define connections for drawing skeletons\n",
        "POSE_CONNECTIONS = [\n",
        "    (11, 13), (13, 15),(15,17),(15,19),(15,21),(16,20),(16,22),(16,18),(12, 14), (14, 16),\n",
        "    (11, 12), (23, 24), (11, 23), (12, 24),\n",
        "    (23, 25), (25, 27), (24, 26), (26, 28),\n",
        "    (27, 31), (28, 32)\n",
        "]\n",
        "\n",
        "HAND_CONNECTIONS = [\n",
        "    (0, 1),(0,17), (1, 2), (2, 3), (3, 4),\n",
        "    (0, 5), (5, 6), (6, 7), (7, 8),\n",
        "    (5, 9), (9, 10), (10, 11), (11, 12),\n",
        "    (9, 13), (13, 14), (14, 15), (15, 16),\n",
        "    (13, 17), (17, 18), (18, 19), (19, 20)\n",
        "]\n",
        "\n",
        "def plot_skeleton(ax, keypoints, connections, label, color):\n",
        "    for i, j in connections:\n",
        "        if i < len(keypoints) and j < len(keypoints):\n",
        "            ax.plot([keypoints[i, 0], keypoints[j, 0]],\n",
        "                    [keypoints[i, 1], keypoints[j, 1]],\n",
        "                    [keypoints[i, 2], keypoints[j, 2]],\n",
        "                    color=color, lw=2)\n",
        "    ax.scatter(keypoints[:, 0], keypoints[:, 1], keypoints[:, 2], label=label, s=10, color=color)\n",
        "    ax.set_title(label)\n",
        "    ax.set_xlabel(\"X\")\n",
        "    ax.set_ylabel(\"Y\")\n",
        "    ax.set_zlabel(\"Z\")\n",
        "    ax.view_init(elev=20, azim=120)\n",
        "\n",
        "# Load transformed file\n",
        "pickle_path = '/content/drive/MyDrive/s0001_f_w000067_transformed.pkl'\n",
        "with open(pickle_path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Choose the frame number\n",
        "frame_num = 123  # <- Change this to visualize different frames\n",
        "\n",
        "pose_kpts = data['pose'][frame_num][:, :3]\n",
        "left_kpts = data['left hand'][frame_num]\n",
        "right_kpts = data['right hand'][frame_num]\n",
        "\n",
        "# Plot each skeleton separately\n",
        "fig = plt.figure(figsize=(18, 5))\n",
        "\n",
        "# Pose\n",
        "ax1 = fig.add_subplot(131, projection='3d')\n",
        "plot_skeleton(ax1, pose_kpts, POSE_CONNECTIONS, label='Pose', color='blue')\n",
        "\n",
        "# Left hand\n",
        "ax2 = fig.add_subplot(132, projection='3d')\n",
        "plot_skeleton(ax2, left_kpts, HAND_CONNECTIONS, label='Left Hand', color='green')\n",
        "\n",
        "# Right hand\n",
        "ax3 = fig.add_subplot(133, projection='3d')\n",
        "plot_skeleton(ax3, right_kpts, HAND_CONNECTIONS, label='Right Hand', color='red')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0AGSIvje4lsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly\n"
      ],
      "metadata": {
        "id": "17DrRM224lp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Define skeleton connections\n",
        "POSE_CONNECTIONS = [\n",
        "    (11, 13), (13, 15), (15, 17), (15, 19), (15, 21), (16, 20), (16, 22), (16, 18),\n",
        "    (12, 14), (14, 16), (11, 12), (23, 24), (11, 23), (12, 24),\n",
        "    (23, 25), (25, 27), (24, 26), (26, 28), (27, 31), (28, 32)\n",
        "]\n",
        "\n",
        "HAND_CONNECTIONS = [\n",
        "    (0, 1), (0, 17), (1, 2), (2, 3), (3, 4),\n",
        "    (0, 5), (5, 6), (6, 7), (7, 8),\n",
        "    (5, 9), (9, 10), (10, 11), (11, 12),\n",
        "    (9, 13), (13, 14), (14, 15), (15, 16),\n",
        "    (13, 17), (17, 18), (18, 19), (19, 20)\n",
        "]\n",
        "\n",
        "def create_skeleton_trace(kpts, connections, color, name):\n",
        "    lines = []\n",
        "    for i, j in connections:\n",
        "        if i < len(kpts) and j < len(kpts):\n",
        "            lines.append(go.Scatter3d(\n",
        "                x=[kpts[i, 0], kpts[j, 0]],\n",
        "                y=[kpts[i, 1], kpts[j, 1]],\n",
        "                z=[kpts[i, 2], kpts[j, 2]],\n",
        "                mode='lines',\n",
        "                line=dict(color=color, width=4),\n",
        "                showlegend=False\n",
        "            ))\n",
        "    joints = go.Scatter3d(\n",
        "        x=kpts[:, 0],\n",
        "        y=kpts[:, 1],\n",
        "        z=kpts[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=4, color=color),\n",
        "        name=f\"{name} joints\",\n",
        "        showlegend=False\n",
        "    )\n",
        "    return lines + [joints]\n",
        "\n",
        "# Load transformed data\n",
        "with open('/content/drive/MyDrive/s0001_f_w000067_transformed.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "frame_num = 105  # ← Change frame number as needed\n",
        "pose_kpts = data['pose'][frame_num][:, :3]\n",
        "left_kpts = data['left hand'][frame_num]\n",
        "right_kpts = data['right hand'][frame_num]\n",
        "\n",
        "# Plot Pose\n",
        "pose_fig = go.Figure(data=create_skeleton_trace(pose_kpts, POSE_CONNECTIONS, 'blue', 'Pose'))\n",
        "pose_fig.update_layout(\n",
        "    title=f\"Pose Skeleton - Frame {frame_num}\",\n",
        "    scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z', aspectmode='data'),\n",
        "    margin=dict(l=0, r=0, t=30, b=0)\n",
        ")\n",
        "pose_fig.show()\n",
        "\n",
        "# Plot Left Hand\n",
        "left_fig = go.Figure(data=create_skeleton_trace(left_kpts, HAND_CONNECTIONS, 'green', 'Left Hand'))\n",
        "left_fig.update_layout(\n",
        "    title=f\"Left Hand Skeleton - Frame {frame_num}\",\n",
        "    scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z', aspectmode='data'),\n",
        "    margin=dict(l=0, r=0, t=30, b=0)\n",
        ")\n",
        "left_fig.show()\n",
        "\n",
        "# Plot Right Hand\n",
        "right_fig = go.Figure(data=create_skeleton_trace(right_kpts, HAND_CONNECTIONS, 'red', 'Right Hand'))\n",
        "right_fig.update_layout(\n",
        "    title=f\"Right Hand Skeleton - Frame {frame_num}\",\n",
        "    scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z', aspectmode='data'),\n",
        "    margin=dict(l=0, r=0, t=30, b=0)\n",
        ")\n",
        "right_fig.show()\n"
      ],
      "metadata": {
        "id": "eL_nbAZS4lnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def normalize_and_align_skeleton(pose, left_hand, right_hand, video_width, video_height):\n",
        "    \"\"\"\n",
        "    Normalize and rotate-align 3D skeletons (pose, left hand, right hand).\n",
        "    Pose is centered at mid-shoulders and scaled to 3x shoulder distance.\n",
        "    Hands are centered at wrist and scaled to finger chain length.\n",
        "    \"\"\"\n",
        "\n",
        "    def normalize_kpts(kpts, part):\n",
        "        if np.isnan(kpts).any():\n",
        "            return kpts  # Skip if any NaNs\n",
        "\n",
        "        if part == 'pose':\n",
        "            # Center at midpoint of shoulders (keypoints 11 and 12)\n",
        "            shoulder_l, shoulder_r = kpts[11], kpts[12]\n",
        "            center = (shoulder_l + shoulder_r) / 2.0\n",
        "            kpts_centered = kpts - center\n",
        "\n",
        "            # Scale = 3 * shoulder distance\n",
        "            scale = np.linalg.norm(shoulder_l - shoulder_r) * 6.0\n",
        "            scale = scale if scale > 0 else 1.0\n",
        "            kpts_scaled = kpts_centered / scale\n",
        "\n",
        "            # Direction = neck - center\n",
        "            direction = kpts[0] - center  # Approx. spine direction\n",
        "        else:\n",
        "            # Hands\n",
        "            center = kpts[0]\n",
        "            kpts_centered = kpts - center\n",
        "\n",
        "            # Scale = wrist to middle fingertip (0→9→10→11→12)\n",
        "            chain = [0, 9, 10, 11, 12]\n",
        "            scale = sum(np.linalg.norm(kpts[chain[i+1]] - kpts[chain[i]]) for i in range(len(chain)-1))\n",
        "            scale = scale if scale > 0 else 1.0\n",
        "            kpts_scaled = kpts_centered / scale\n",
        "\n",
        "            direction = kpts[12] - kpts[0]  # Middle finger tip direction\n",
        "\n",
        "        # Normalize and rotate to align \"up\" to +Y\n",
        "        direction /= np.linalg.norm(direction) + 1e-6\n",
        "        target = np.array([0, 1, 0])  # Up\n",
        "\n",
        "        v = np.cross(direction, target)\n",
        "        s = np.linalg.norm(v)\n",
        "        c = np.dot(direction, target)\n",
        "        if s == 0:\n",
        "            R = np.eye(3)\n",
        "        else:\n",
        "            vx = np.array([[0, -v[2], v[1]],\n",
        "                           [v[2], 0, -v[0]],\n",
        "                           [-v[1], v[0], 0]])\n",
        "            R = np.eye(3) + vx + (vx @ vx) * ((1 - c) / (s ** 2))\n",
        "\n",
        "        kpts_rotated = kpts_scaled @ R.T\n",
        "        return kpts_rotated\n",
        "\n",
        "    pose_new = normalize_kpts(pose, 'pose') if pose is not None else None\n",
        "    left_new = normalize_kpts(left_hand, 'hand') if left_hand is not None else None\n",
        "    right_new = normalize_kpts(right_hand, 'hand') if right_hand is not None else None\n",
        "    return pose_new, left_new, right_new\n",
        "\n",
        "\n",
        "# Load original file\n",
        "input_path = '/content/drive/MyDrive/s0001_f_w000067.pkl'\n",
        "with open(input_path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "video_height = data['Video height']\n",
        "video_width = data['Video width']\n",
        "pose_data = data['pose']\n",
        "left_data = data['left hand']\n",
        "right_data = data['right hand']\n",
        "\n",
        "transformed_pose = []\n",
        "transformed_left = []\n",
        "transformed_right = []\n",
        "\n",
        "for i in tqdm(range(len(pose_data))):\n",
        "    # Scale to pixel space\n",
        "    pose = pose_data[i][:, :3] * [video_width, video_height, video_width]\n",
        "    left = left_data[i] * [video_width, video_height, video_width]\n",
        "    right = right_data[i] * [video_width, video_height, video_width]\n",
        "\n",
        "    # Normalize and rotate\n",
        "    p_t, l_t, r_t = normalize_and_align_skeleton(pose, left, right, video_width, video_height)\n",
        "\n",
        "    # Reattach confidence for pose\n",
        "    if p_t is not None:\n",
        "        conf = pose_data[i][:, 3:4]  # Keep original confidence\n",
        "        p_t = np.concatenate([p_t, conf], axis=-1)\n",
        "\n",
        "    transformed_pose.append(p_t)\n",
        "    transformed_left.append(l_t)\n",
        "    transformed_right.append(r_t)\n",
        "\n",
        "# Save transformed data\n",
        "transformed_data = {\n",
        "    'Video height': video_height,\n",
        "    'Video width': video_width,\n",
        "    'pose': np.array(transformed_pose),\n",
        "    'left hand': np.array(transformed_left),\n",
        "    'right hand': np.array(transformed_right),\n",
        "}\n",
        "\n",
        "output_path = '/content/drive/MyDrive/s0001_f_w000067_transformed_c.pkl'\n",
        "with open(output_path, 'wb') as f:\n",
        "    pickle.dump(transformed_data, f)\n",
        "\n",
        "print(\"✅ Transformed keypoints saved to:\", output_path)\n"
      ],
      "metadata": {
        "id": "zK_7Epim4lk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Define skeleton connections\n",
        "POSE_CONNECTIONS = [\n",
        "    (11, 13), (13, 15), (15, 17), (15, 19), (15, 21), (16, 20), (16, 22), (16, 18),\n",
        "    (12, 14), (14, 16), (11, 12), (23, 24), (11, 23), (12, 24),\n",
        "    (23, 25), (25, 27), (24, 26), (26, 28), (27, 31), (28, 32)\n",
        "]\n",
        "\n",
        "HAND_CONNECTIONS = [\n",
        "    (0, 1), (0, 17), (1, 2), (2, 3), (3, 4),\n",
        "    (0, 5), (5, 6), (6, 7), (7, 8),\n",
        "    (5, 9), (9, 10), (10, 11), (11, 12),\n",
        "    (9, 13), (13, 14), (14, 15), (15, 16),\n",
        "    (13, 17), (17, 18), (18, 19), (19, 20)\n",
        "]\n",
        "\n",
        "def create_skeleton_trace(kpts, connections, color, name):\n",
        "    lines = []\n",
        "    for i, j in connections:\n",
        "        if i < len(kpts) and j < len(kpts) and not (np.isnan(kpts[i]).any() or np.isnan(kpts[j]).any()):\n",
        "            lines.append(go.Scatter3d(\n",
        "                x=[kpts[i, 0], kpts[j, 0]],\n",
        "                y=[kpts[i, 1], kpts[j, 1]],\n",
        "                z=[kpts[i, 2], kpts[j, 2]],\n",
        "                mode='lines',\n",
        "                line=dict(color=color, width=4),\n",
        "                showlegend=False\n",
        "            ))\n",
        "    # Plot joints\n",
        "    valid = ~np.isnan(kpts).any(axis=1)\n",
        "    joints = go.Scatter3d(\n",
        "        x=kpts[valid, 0],\n",
        "        y=kpts[valid, 1],\n",
        "        z=kpts[valid, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=4, color=color),\n",
        "        name=f\"{name} joints\",\n",
        "        showlegend=False\n",
        "    )\n",
        "    return lines + [joints]\n",
        "\n",
        "# Load transformed data\n",
        "with open('/content/drive/MyDrive/s0001_f_w000067_transformed_c.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "frame_num = 105  # Change as needed\n",
        "pose_kpts = data['pose'][frame_num][:, :3]\n",
        "left_kpts = data['left hand'][frame_num]\n",
        "right_kpts = data['right hand'][frame_num]\n",
        "\n",
        "# Visualize pose\n",
        "pose_fig = go.Figure(data=create_skeleton_trace(pose_kpts, POSE_CONNECTIONS, 'blue', 'Pose'))\n",
        "pose_fig.update_layout(\n",
        "    title=f\"Normalized Pose Skeleton - Frame {frame_num}\",\n",
        "    scene=dict(\n",
        "        xaxis_title='X', yaxis_title='Y', zaxis_title='Z',\n",
        "        aspectmode='data'\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, t=30, b=0)\n",
        ")\n",
        "pose_fig.show()\n",
        "\n",
        "# Visualize left hand\n",
        "left_fig = go.Figure(data=create_skeleton_trace(left_kpts, HAND_CONNECTIONS, 'green', 'Left Hand'))\n",
        "left_fig.update_layout(\n",
        "    title=f\"Normalized Left Hand Skeleton - Frame {frame_num}\",\n",
        "    scene=dict(\n",
        "        xaxis_title='X', yaxis_title='Y', zaxis_title='Z',\n",
        "        aspectmode='data'\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, t=30, b=0)\n",
        ")\n",
        "left_fig.show()\n",
        "\n",
        "# Visualize right hand\n",
        "right_fig = go.Figure(data=create_skeleton_trace(right_kpts, HAND_CONNECTIONS, 'red', 'Right Hand'))\n",
        "right_fig.update_layout(\n",
        "    title=f\"Normalized Right Hand Skeleton - Frame {frame_num}\",\n",
        "    scene=dict(\n",
        "        xaxis_title='X', yaxis_title='Y', zaxis_title='Z',\n",
        "        aspectmode='data'\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, t=30, b=0)\n",
        ")\n",
        "right_fig.show()\n"
      ],
      "metadata": {
        "id": "CsfsZSkT4liO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import mediapipe as mp\n",
        "\n",
        "# Initialize MediaPipe Holistic\n",
        "mp_holistic = mp.solutions.holistic\n",
        "holistic = mp_holistic.Holistic(static_image_mode=False)\n",
        "\n",
        "# Path to videos directory\n",
        "video_dir = \"/content/drive/MyDrive/data_composite_01/data/\"\n",
        "output_pkl = \"/content/drive/MyDrive/all_transformed_keypoints.pkl\"\n",
        "\n",
        "# Dictionary to hold all keypoints\n",
        "all_keypoints = {}\n",
        "\n",
        "# Normalize and rotate-align skeletons\n",
        "def normalize_and_align_skeleton(pose, left_hand, right_hand, video_width, video_height):\n",
        "    def normalize_kpts(kpts, part):\n",
        "        if np.isnan(kpts).any():\n",
        "            return None\n",
        "\n",
        "        if part == 'pose':\n",
        "            shoulder_l, shoulder_r = kpts[11], kpts[12]\n",
        "            center = (shoulder_l + shoulder_r) / 2.0\n",
        "            kpts_centered = kpts - center\n",
        "            scale = np.linalg.norm(shoulder_l - shoulder_r) * 6.0\n",
        "            scale = scale if scale > 0 else 1.0\n",
        "            kpts_scaled = kpts_centered / scale\n",
        "            direction = kpts[0] - center\n",
        "        else:\n",
        "            center = kpts[0]\n",
        "            kpts_centered = kpts - center\n",
        "            chain = [0, 9, 10, 11, 12]\n",
        "            scale = sum(np.linalg.norm(kpts[chain[i+1]] - kpts[chain[i]]) for i in range(len(chain)-1))\n",
        "            scale = scale if scale > 0 else 1.0\n",
        "            kpts_scaled = kpts_centered / scale\n",
        "            direction = kpts[12] - kpts[0]\n",
        "\n",
        "        direction /= np.linalg.norm(direction) + 1e-6\n",
        "        target = np.array([0, 1, 0])\n",
        "        v = np.cross(direction, target)\n",
        "        s = np.linalg.norm(v)\n",
        "        c = np.dot(direction, target)\n",
        "\n",
        "        if s == 0:\n",
        "            R = np.eye(3)\n",
        "        else:\n",
        "            vx = np.array([\n",
        "                [0, -v[2], v[1]],\n",
        "                [v[2], 0, -v[0]],\n",
        "                [-v[1], v[0], 0]\n",
        "            ])\n",
        "            R = np.eye(3) + vx + (vx @ vx) * ((1 - c) / (s ** 2))\n",
        "\n",
        "        return kpts_scaled @ R.T\n",
        "\n",
        "    pose_new = normalize_kpts(pose, 'pose') if pose is not None else None\n",
        "    left_new = normalize_kpts(left_hand, 'hand') if left_hand is not None else None\n",
        "    right_new = normalize_kpts(right_hand, 'hand') if right_hand is not None else None\n",
        "    return pose_new, left_new, right_new\n",
        "\n",
        "# Process each video\n",
        "for video_name in tqdm(os.listdir(video_dir)):\n",
        "    if not video_name.endswith((\".mp4\", \".avi\", \".mov\")):\n",
        "        continue\n",
        "\n",
        "    video_path = os.path.join(video_dir, video_name)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    video_id = os.path.splitext(video_name)[0]\n",
        "\n",
        "    frame_id = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        result = holistic.process(rgb)\n",
        "\n",
        "        # Extract landmarks\n",
        "        pose = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in result.pose_landmarks.landmark]) if result.pose_landmarks else None\n",
        "        left = np.array([[lm.x, lm.y, lm.z] for lm in result.left_hand_landmarks.landmark]) if result.left_hand_landmarks else None\n",
        "        right = np.array([[lm.x, lm.y, lm.z] for lm in result.right_hand_landmarks.landmark]) if result.right_hand_landmarks else None\n",
        "\n",
        "        # Skip if any part is missing\n",
        "        if pose is None or left is None or right is None:\n",
        "            frame_id += 1\n",
        "            continue\n",
        "\n",
        "        # Scale to pixel space\n",
        "        pose_scaled = pose[:, :3] * [video_width, video_height, video_width]\n",
        "        left_scaled = left * [video_width, video_height, video_width]\n",
        "        right_scaled = right * [video_width, video_height, video_width]\n",
        "\n",
        "        # Normalize and align\n",
        "        p_norm, l_norm, r_norm = normalize_and_align_skeleton(pose_scaled, left_scaled, right_scaled, video_width, video_height)\n",
        "\n",
        "        # Skip if normalization fails or any NaNs appear\n",
        "        if (\n",
        "            p_norm is None or np.isnan(p_norm).any() or\n",
        "            l_norm is None or np.isnan(l_norm).any() or\n",
        "            r_norm is None or np.isnan(r_norm).any()\n",
        "        ):\n",
        "            frame_id += 1\n",
        "            continue\n",
        "\n",
        "        # Reattach confidence\n",
        "        p_norm = np.concatenate([p_norm, pose[:, 3:4]], axis=-1)\n",
        "\n",
        "        # Store in dictionary\n",
        "        key = f\"{video_id}_{frame_id}\"\n",
        "        all_keypoints[key] = {\n",
        "            \"Video height\": video_height,\n",
        "            \"Video width\": video_width,\n",
        "            \"pose\": p_norm,\n",
        "            \"left hand\": l_norm,\n",
        "            \"right hand\": r_norm\n",
        "        }\n",
        "\n",
        "        frame_id += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "# Save all keypoints to a single .pkl file\n",
        "with open(output_pkl, 'wb') as f:\n",
        "    pickle.dump(all_keypoints, f)\n",
        "\n",
        "print(\"✅ Saved all transformed and normalized keypoints to:\", output_pkl)\n"
      ],
      "metadata": {
        "id": "6DPujG0g4lfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AUTOENCODER TRAINING"
      ],
      "metadata": {
        "id": "-i02pBWu5BSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📦 Imports\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "# 📍 Path to your saved all_transformed_keypoints.pkl\n",
        "pkl_path = '/content/drive/MyDrive/all_transformed_keypoints.pkl'\n",
        "\n",
        "# 📖 Load keypoints dictionary\n",
        "with open(pkl_path, 'rb') as f:\n",
        "    data_dict = pickle.load(f)\n",
        "\n",
        "print(f\"✅ Loaded {len(data_dict)} keypoint samples.\")"
      ],
      "metadata": {
        "id": "OciedgxN4lc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These are the keypoint indices for upper body:\n",
        "upper_body_idx = [0, 2, 5 ,11, 12, 13, 14, 15, 16]"
      ],
      "metadata": {
        "id": "tUAxMyrZ4laE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------- 2. Custom Dataset --------------------\n",
        "class KeypointDataset(Dataset):\n",
        "    def __init__(self, data_dict):\n",
        "        self.keys = list(data_dict.keys())\n",
        "        self.data = data_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        key = self.keys[idx]\n",
        "        sample = self.data[key]\n",
        "        return {\n",
        "            'pose': torch.tensor(sample['pose'], dtype=torch.float32),\n",
        "            'left': torch.tensor(sample['left hand'], dtype=torch.float32),\n",
        "            'right': torch.tensor(sample['right hand'], dtype=torch.float32),\n",
        "        }\n",
        "\n",
        "dataset = KeypointDataset(data_dict)\n",
        "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n"
      ],
      "metadata": {
        "id": "3LX2GhTB3lar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim=16):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Encoder: input → 128 → 64 → latent\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, latent_dim)  # latent vector\n",
        "        )\n",
        "\n",
        "        # Decoder: latent → 64 → 128 → output\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, input_dim)  # reconstruct original input\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        recon = self.decoder(z)\n",
        "        return recon\n"
      ],
      "metadata": {
        "id": "v0cOk8A93lYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Autoencoderpose(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim=16):\n",
        "        super(Autoencoderpose, self).__init__()\n",
        "\n",
        "        # Encoder: input → 128 → 64 → latent\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(32, latent_dim)  # latent vector\n",
        "        )\n",
        "\n",
        "        # Decoder: latent → 64 → 128 → output\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 32),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(64, input_dim)  # reconstruct original input\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        recon = self.decoder(z)\n",
        "        return recon\n"
      ],
      "metadata": {
        "id": "fGLlTeX33lUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class KeypointDataset(Dataset):\n",
        "    def __init__(self, data_dict):\n",
        "        self.data = data_dict\n",
        "        self.keys = list(data_dict.keys())\n",
        "        self.upper_body_idx = [0, 2, 5, 11, 12, 13, 14, 15, 16]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        key = self.keys[idx]\n",
        "        sample = self.data[key]\n",
        "\n",
        "        # Safely extract and reshape keypoints\n",
        "        pose = sample['pose'][self.upper_body_idx, :3].reshape(-1)      # shape: (27,)\n",
        "        # pose = sample['pose'][:, :3].reshape(-1)  # shape: (99,)\n",
        "\n",
        "        left = sample['left hand'][:, :3].reshape(-1)                   # shape: (63,)\n",
        "        right = sample['right hand'][:, :3].reshape(-1)                 # shape: (63,)\n",
        "\n",
        "        return (\n",
        "            torch.from_numpy(pose).float(),\n",
        "            torch.from_numpy(left).float(),\n",
        "            torch.from_numpy(right).float()\n",
        "        )\n"
      ],
      "metadata": {
        "id": "BT4kW4xW3lKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import pickle\n",
        "# Load from file\n",
        "with open('/content/drive/MyDrive/all_transformed_keypoints.pkl', 'rb') as f:\n",
        "    all_data = pickle.load(f)\n",
        "\n",
        "# Dataset and Dataloader\n",
        "dataset = KeypointDataset(all_data)\n",
        "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n"
      ],
      "metadata": {
        "id": "m5tqp-8S5Oib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pose, left, right = dataset[0]\n",
        "print(pose.shape, left.shape, right.shape)\n",
        "print(type(pose), type(left), type(right))\n"
      ],
      "metadata": {
        "id": "DnpeYqmr5Ofs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample = dataset[0]  # take the first sample\n",
        "\n",
        "# pose_dim = sample['pose'].shape[0]\n",
        "# left_dim = sample['left'].shape[0]\n",
        "# right_dim = sample['right'].shape[0]\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "pose, left, right = dataset[0]  # unpack the tuple\n",
        "\n",
        "pose_dim = pose.shape[0]\n",
        "left_dim = left.shape[0]\n",
        "right_dim = right.shape[0]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "GWTF0f3X5Oc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pose_ae = Autoencoderpose(input_dim=99, latent_dim=16).to(device)\n",
        "pose_ae = Autoencoderpose(input_dim=27, latent_dim=16).to(device)\n",
        "left_ae = Autoencoder(input_dim=63, latent_dim=16).to(device)\n",
        "right_ae = Autoencoder(input_dim=63, latent_dim=16).to(device)\n"
      ],
      "metadata": {
        "id": "gxVbe0JM5OaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer_pose = torch.optim.Adam(pose_ae.parameters(), lr=1e-3)\n",
        "optimizer_left = torch.optim.Adam(left_ae.parameters(), lr=1e-3)\n",
        "optimizer_right = torch.optim.Adam(right_ae.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "ReWLQWUa5OYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Scheduler setup (after optimizer_pose)\n",
        "scheduler_pose = ReduceLROnPlateau(\n",
        "    optimizer_pose,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    #patience=100,\n",
        "    patience=50,\n",
        "    #threshold=1e-6,\n",
        "    threshold = 1e-5,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "DSaCvX_E5OVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Scheduler setup (after optimizer_pose)\n",
        "scheduler_left = ReduceLROnPlateau(\n",
        "    optimizer_left,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    #patience=100,\n",
        "    patience=50,\n",
        "    #threshold=1e-6,\n",
        "    threshold = 1e-5,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "5-oVVgrV5OSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pose, left, right = dataset[0]\n",
        "print(type(pose), type(left), type(right))  # All should be <class 'torch.Tensor'>\n"
      ],
      "metadata": {
        "id": "LnbQZtLl5OQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pose, _, _ = dataset[0]\n",
        "print(pose)\n"
      ],
      "metadata": {
        "id": "lwT4CISl5ONO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 4000\n",
        "patience = 50\n",
        "best_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "early_stop = False\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss_pose = 0\n",
        "\n",
        "    for pose, left, right in dataloader:\n",
        "        pose = pose.to(device)\n",
        "\n",
        "        optimizer_pose.zero_grad()\n",
        "        recon_pose = pose_ae(pose)\n",
        "        loss_pose = criterion(recon_pose, pose)\n",
        "        loss_pose.backward()\n",
        "        optimizer_pose.step()\n",
        "\n",
        "        total_loss_pose += loss_pose.item()\n",
        "\n",
        "    avg_loss_pose = total_loss_pose / len(dataloader)\n",
        "    scheduler_pose.step(avg_loss_pose)\n",
        "\n",
        "    current_lr = optimizer_pose.param_groups[0]['lr']\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Total Pose Loss: {total_loss_pose:.6f} | LR: {current_lr:.6f}\")\n",
        "\n",
        "    # Early Stopping Check\n",
        "    if avg_loss_pose < best_loss - 1e-10:  # small threshold to count only meaningful improvements\n",
        "        best_loss = avg_loss_pose\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(pose_ae.state_dict(), 'best_pose_model.pth')  # Optional: save best model\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"⏹️ Early stopping triggered after {epoch+1} epochs! Best loss: {best_loss:.6f}\")\n",
        "        early_stop = True\n",
        "        break\n",
        "\n",
        "if not early_stop:\n",
        "    print(\"✅ Training completed without early stopping.\")\n",
        "else:\n",
        "    print(\"📦 Loaded best model for further use.\")\n",
        "    pose_ae.load_state_dict(torch.load('best_pose_model.pth'))  # Optional: reload best weights\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# 📂 Save directory (in your GDrive)\n",
        "save_dir = '/content/drive/MyDrive/autoencoder_models/'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# ✅ Save each model\n",
        "torch.save(pose_ae.state_dict(), os.path.join(save_dir, 'pose_aese.pth'))\n",
        "print(\"✅ All 3 Autoencoder models saved to your Google Drive successfully!\")"
      ],
      "metadata": {
        "id": "vHG6kxuB5OKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 4000\n",
        "patience = 500\n",
        "best_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "early_stop = False\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss_left = 0\n",
        "\n",
        "    for pose, left, right in dataloader:\n",
        "        left = left.to(device)\n",
        "\n",
        "        optimizer_left.zero_grad()\n",
        "        recon_left = left_ae(left)\n",
        "        loss_left = criterion(recon_left, left)\n",
        "        loss_left.backward()\n",
        "        optimizer_left.step()\n",
        "\n",
        "        total_loss_left += loss_left.item()\n",
        "\n",
        "    avg_loss_left = total_loss_left / len(dataloader)\n",
        "    scheduler_left.step(avg_loss_left)\n",
        "\n",
        "    current_lr = optimizer_left.param_groups[0]['lr']\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Total left Loss: {total_loss_left:.10f} | LR: {current_lr:.10f}\")\n",
        "\n",
        "    # Early Stopping Check\n",
        "    if avg_loss_left < best_loss - 1e-100:  # small threshold to count only meaningful improvements\n",
        "        best_loss = avg_loss_left\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(left_ae.state_dict(), 'best_left_model.pth')  # Optional: save best model\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"⏹️ Early stopping triggered after {epoch+1} epochs! Best loss: {best_loss:.6f}\")\n",
        "        early_stop = True\n",
        "        break\n",
        "\n",
        "if not early_stop:\n",
        "    print(\"✅ Training completed without early stopping.\")\n",
        "else:\n",
        "    print(\"📦 Loaded best model for further use.\")\n",
        "    left_ae.load_state_dict(torch.load('best_left_model.pth'))  # Optional: reload best weights\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# 📂 Save directory (in your GDrive)\n",
        "save_dir = '/content/drive/MyDrive/autoencoder_models/'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# ✅ Save each model\n",
        "torch.save(left_ae.state_dict(), os.path.join(save_dir, 'left_aenew.pth'))\n",
        "print(\"✅ All 3 Autoencoder models saved to your Google Drive successfully!\")"
      ],
      "metadata": {
        "id": "vI-HUZe65OIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 300\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss_pose = 0\n",
        "    total_loss_left = 0\n",
        "    total_loss_right = 0\n",
        "\n",
        "    for pose, left, right in dataloader:\n",
        "        pose, left, right = pose.to(device), left.to(device), right.to(device)\n",
        "\n",
        "\n",
        "\n",
        "        # Right Hand AE\n",
        "        optimizer_right.zero_grad()\n",
        "        recon_right = right_ae(right)\n",
        "        loss_right = criterion(recon_right, right)\n",
        "        loss_right.backward()\n",
        "        optimizer_right.step()\n",
        "\n",
        "        total_loss_right += loss_right.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Right Loss: {total_loss_right:.5f}\")\n"
      ],
      "metadata": {
        "id": "1ez-BCt95OFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# 📂 Save directory (in your GDrive)\n",
        "save_dir = '/content/drive/MyDrive/autoencoder_models/'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# ✅ Save each model\n",
        "torch.save(pose_ae.state_dict(), os.path.join(save_dir, 'pose_aese.pth'))\n",
        "torch.save(left_ae.state_dict(), os.path.join(save_dir, 'left_hand_ae4.pth'))\n",
        "torch.save(right_ae.state_dict(), os.path.join(save_dir, 'right_hand_ae4.pth'))\n",
        "\n",
        "print(\"✅ All 3 Autoencoder models saved to your Google Drive successfully!\")\n"
      ],
      "metadata": {
        "id": "cWXYiocB5OCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================== Imports ===================\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import os\n",
        "\n",
        "# =================== Define Skeleton Connections ===================\n",
        "UPPER_BODY_CONNECTIONS = [\n",
        "    (0, 1), (0, 2), (0, 3), (0, 4),\n",
        "    (3,5), (4,6),\n",
        "    (5,7), (6,8)\n",
        "]\n",
        "# UPPER_BODY_CONNECTIONS = [\n",
        "#     (0, 1),  # nose to right eye\n",
        "#     (0, 2),  # nose to left eye\n",
        "#     (0, 3),  # nose to left shoulder\n",
        "#     (0, 4),  # nose to right shoulder\n",
        "#     (3, 5),  # left shoulder to left elbow\n",
        "#     (5, 6),  # left elbow to left wrist\n",
        "#     (4, 7),  # right shoulder to right elbow\n",
        "#     (7, 8),  # right elbow to right wrist\n",
        "# ]\n",
        "HAND_CONNECTIONS = [\n",
        "    (0, 1), (0, 17), (1, 2), (2, 3), (3, 4),\n",
        "    (0, 5), (5, 6), (6, 7), (7, 8),\n",
        "    (5, 9), (9, 10), (10, 11), (11, 12),\n",
        "    (9, 13), (13, 14), (14, 15), (15, 16),\n",
        "    (13, 17), (17, 18), (18, 19), (19, 20)\n",
        "]\n",
        "\n",
        "def create_skeleton_trace(kpts, connections, color, name):\n",
        "    lines = []\n",
        "    for i, j in connections:\n",
        "        if i < len(kpts) and j < len(kpts) and not (np.isnan(kpts[i]).any() or np.isnan(kpts[j]).any()):\n",
        "            lines.append(go.Scatter3d(\n",
        "                x=[kpts[i, 0], kpts[j, 0]],\n",
        "                y=[kpts[i, 1], kpts[j, 1]],\n",
        "                z=[kpts[i, 2], kpts[j, 2]],\n",
        "                mode='lines',\n",
        "                line=dict(color=color, width=4),\n",
        "                showlegend=False\n",
        "            ))\n",
        "    valid = ~np.isnan(kpts).any(axis=1)\n",
        "    joints = go.Scatter3d(\n",
        "        x=kpts[valid, 0],\n",
        "        y=kpts[valid, 1],\n",
        "        z=kpts[valid, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=4, color=color),\n",
        "        name=f\"{name} joints\",\n",
        "        showlegend=False\n",
        "    )\n",
        "    return lines + [joints]\n",
        "\n",
        "# =================== Autoencoder Definition ===================\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim=16):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, latent_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        recon = self.decoder(z)\n",
        "        return recon\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Autoencoderpose(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim=16):\n",
        "        super(Autoencoderpose, self).__init__()\n",
        "\n",
        "        # Encoder: input → 128 → 64 → latent\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(32, latent_dim)  # latent vector\n",
        "        )\n",
        "\n",
        "        # Decoder: latent → 64 → 128 → output\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 32),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(64, input_dim)  # reconstruct original input\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        recon = self.decoder(z)\n",
        "        return recon\n",
        "\n",
        "# =================== Load Data ===================\n",
        "with open('/content/drive/MyDrive/all_transformed_keypoints.pkl', 'rb') as f:\n",
        "    all_data = pickle.load(f)\n",
        "\n",
        "print(f\"✅ Loaded {len(all_data)} frames from all_transformed_keypoints.pkl.\")\n",
        "\n",
        "# =================== Load Models ===================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "pose_ae = Autoencoderpose(input_dim=27, latent_dim=16).to(device)      # 9 upper-body keypoints × 3D = 27\n",
        "left_ae = Autoencoder(input_dim=63, latent_dim=16).to(device)      # 21 hand kpts × 3D\n",
        "right_ae = Autoencoder(input_dim=63, latent_dim=16).to(device)\n",
        "\n",
        "# Load trained weights\n",
        "pose_ae.load_state_dict(torch.load('/content/drive/MyDrive/autoencoder_models/pose_aesest.pth', map_location=device))\n",
        "left_ae.load_state_dict(torch.load('/content/drive/MyDrive/autoencoder_models/left_hand_ae4.pth', map_location=device))\n",
        "right_ae.load_state_dict(torch.load('/content/drive/MyDrive/autoencoder_models/right_hand_ae4.pth', map_location=device))\n",
        "\n",
        "pose_ae.eval()\n",
        "left_ae.eval()\n",
        "right_ae.eval()\n",
        "\n",
        "print(\"✅ Loaded all 3 Autoencoder models successfully!\")\n",
        "\n",
        "# =================== Choose a Frame to Visualize ===================\n",
        "video_id = 's0001_f_w000067'\n",
        "frame_number = 105   # 🔥 Change this to visualize a different frame\n",
        "frame_key = f\"{video_id}_{frame_number}\"\n",
        "\n",
        "if frame_key not in all_data:\n",
        "    raise ValueError(f\"Frame key '{frame_key}' not found in data!\")\n",
        "\n",
        "# =================== Select Keypoints ===================\n",
        "# Pose: select only 9 upper body keypoints\n",
        "#upper_body_idx = [0, 2 , 5, 11, 12, 13, 15, 14, 16]  # e.g. [nose, shoulders, hips, elbows, wrists]\n",
        "pose_kpts = all_data[frame_key]['pose'][upper_body_idx, :3]\n",
        "left_hand_kpts = all_data[frame_key]['left hand'][:, :3]\n",
        "right_hand_kpts = all_data[frame_key]['right hand'][:, :3]\n",
        "# pose_raw = all_data[frame_key]['pose'][upper_body_idx, :3]\n",
        "# left_hand_raw = all_data[frame_key]['left hand'][:, :3]\n",
        "# right_hand_raw = all_data[frame_key]['right hand'][:, :3]\n",
        "\n",
        "# # 🔄 Normalize and align all 3 skeletons\n",
        "# pose_kpts, left_hand_kpts, right_hand_kpts = normalize_and_align_skeleton(pose_raw, left_hand_raw, right_hand_raw)\n",
        "\n",
        "# =================== Run Autoencoders ===================\n",
        "pose_input = torch.tensor(pose_kpts.flatten(), dtype=torch.float32).unsqueeze(0).to(device)\n",
        "left_input = torch.tensor(left_hand_kpts.flatten(), dtype=torch.float32).unsqueeze(0).to(device)\n",
        "right_input = torch.tensor(right_hand_kpts.flatten(), dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "pose_recon = pose_ae(pose_input).squeeze(0).cpu().detach().numpy().reshape(-1, 3)\n",
        "left_recon = left_ae(left_input).squeeze(0).cpu().detach().numpy().reshape(-1, 3)\n",
        "right_recon = right_ae(right_input).squeeze(0).cpu().detach().numpy().reshape(-1, 3)\n",
        "\n",
        "# =================== Plot Reconstructed Skeletons ===================\n",
        "pose_fig = go.Figure(data=create_skeleton_trace(pose_recon, UPPER_BODY_CONNECTIONS, 'blue', 'Pose'))\n",
        "pose_fig.update_layout(\n",
        "    title=f\"Reconstructed Pose Skeleton - {frame_key}\",\n",
        "    scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z', aspectmode='data'),\n",
        "    margin=dict(l=0, r=0, t=30, b=0)\n",
        ")\n",
        "pose_fig.show()\n",
        "\n",
        "left_fig = go.Figure(data=create_skeleton_trace(left_recon, HAND_CONNECTIONS, 'green', 'Left Hand'))\n",
        "left_fig.update_layout(\n",
        "    title=f\"Reconstructed Left Hand Skeleton - {frame_key}\",\n",
        "    scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z', aspectmode='data'),\n",
        "    margin=dict(l=0, r=0, t=30, b=0)\n",
        ")\n",
        "left_fig.show()\n",
        "\n",
        "right_fig = go.Figure(data=create_skeleton_trace(right_recon, HAND_CONNECTIONS, 'red', 'Right Hand'))\n",
        "right_fig.update_layout(\n",
        "    title=f\"Reconstructed Right Hand Skeleton - {frame_key}\",\n",
        "    scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z', aspectmode='data'),\n",
        "    margin=dict(l=0, r=0, t=30, b=0)\n",
        ")\n",
        "right_fig.show()\n",
        "\n",
        "print(\"✅ Finished plotting reconstructed skeletons!\")\n"
      ],
      "metadata": {
        "id": "MOBYLnsO5N_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#KMEANS CLUSTERING"
      ],
      "metadata": {
        "id": "hPFoSBol59kM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# # Autoencoder class\n",
        "# class Autoencoder(nn.Module):\n",
        "#     def __init__(self, input_dim, latent_dim=16):\n",
        "#         super(Autoencoder, self).__init__()\n",
        "#         self.encoder = nn.Sequential(\n",
        "#             nn.Linear(input_dim, 128),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(128, 64),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(64, latent_dim)\n",
        "#         )\n",
        "#         self.decoder = nn.Sequential(\n",
        "#             nn.Linear(latent_dim, 64),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(64, 128),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(128, input_dim)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         z = self.encoder(x)\n",
        "#         recon = self.decoder(z)\n",
        "#         return recon\n",
        "\n",
        "# Load models\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pose_ae = Autoencoderpose(27, 16).to(device)\n",
        "left_ae = Autoencoder(63, 16).to(device)\n",
        "right_ae = Autoencoder(63, 16).to(device)\n",
        "\n",
        "pose_ae.load_state_dict(torch.load(\"/content/drive/MyDrive/autoencoder_models/pose_aesest.pth\",map_location=device))\n",
        "left_ae.load_state_dict(torch.load(\"/content/drive/MyDrive/autoencoder_models/left_hand_ae4.pth\",map_location=device))\n",
        "right_ae.load_state_dict(torch.load(\"/content/drive/MyDrive/autoencoder_models/right_hand_ae4.pth\",map_location=device))\n",
        "\n",
        "pose_ae.eval()\n",
        "left_ae.eval()\n",
        "right_ae.eval()\n"
      ],
      "metadata": {
        "id": "uCdYPXaJ5N88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/all_transformed_keypoints.pkl', 'rb') as f:\n",
        "    all_data = pickle.load(f)\n",
        "\n",
        "print(f\"Total frames: {len(all_data)}\")\n"
      ],
      "metadata": {
        "id": "fM3oeEBD5N6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "pose_latents = []\n",
        "left_latents = []\n",
        "right_latents = []\n",
        "\n",
        "# upper_body_idx = [0, 11, 12, 23, 24, 13, 15, 14, 16]  # Use your training indices\n",
        "upper_body_idx = [0, 2, 5, 11, 12, 13, 14, 15, 16]\n",
        "\n",
        "frame_keys = sorted(all_data.keys())  # Define this once, outside the loop\n",
        "\n",
        "for frame_key in frame_keys:  # Loop over each frame key\n",
        "    pose_kpts = all_data[frame_key]['pose'][upper_body_idx, :3]\n",
        "    left_kpts = all_data[frame_key]['left hand'][:, :3]\n",
        "    right_kpts = all_data[frame_key]['right hand'][:, :3]\n",
        "\n",
        "    # Flatten and convert to tensor\n",
        "    pose_tensor = torch.tensor(pose_kpts.flatten(), dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    left_tensor = torch.tensor(left_kpts.flatten(), dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    right_tensor = torch.tensor(right_kpts.flatten(), dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pose_z = pose_ae.encoder(pose_tensor).cpu().numpy().squeeze()\n",
        "        left_z = left_ae.encoder(left_tensor).cpu().numpy().squeeze()\n",
        "        right_z = right_ae.encoder(right_tensor).cpu().numpy().squeeze()\n",
        "\n",
        "    pose_latents.append(pose_z)\n",
        "    left_latents.append(left_z)\n",
        "    right_latents.append(right_z)\n",
        "\n",
        "pose_latents = np.vstack(pose_latents)\n",
        "left_latents = np.vstack(left_latents)\n",
        "right_latents = np.vstack(right_latents)\n",
        "print(\"✅ Latent vectors extracted.\")\n"
      ],
      "metadata": {
        "id": "Wu_bf7I85N3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "n_clusters = 50  # You can change this\n",
        "\n",
        "pose_kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "\n",
        "\n",
        "pose_labels = pose_kmeans.fit_predict(pose_latents)\n",
        "\n",
        "\n",
        "print(\"✅ Clustering done.\")\n"
      ],
      "metadata": {
        "id": "Rd_jhvnG5N1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "n_clusters = 100  # You can change this\n",
        "\n",
        "\n",
        "left_kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "\n",
        "\n",
        "left_labels = left_kmeans.fit_predict(left_latents)\n",
        "\n",
        "\n",
        "print(\"✅ Clustering done.\")\n"
      ],
      "metadata": {
        "id": "tQh8wcsY5NyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "n_clusters = 100  # You can change this\n",
        "\n",
        "\n",
        "right_kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "\n",
        "\n",
        "right_labels = right_kmeans.fit_predict(right_latents)\n",
        "\n",
        "print(\"✅ Clustering done.\")\n"
      ],
      "metadata": {
        "id": "OI4Uq7885Nvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Example: your clustering outputs\n",
        "pose_data = {\n",
        "    'labels': pose_labels,                # np.ndarray or list\n",
        "    'kmeans': pose_kmeans,                # KMeans model\n",
        "    'latents': pose_latents               # optionally include\n",
        "}\n",
        "\n",
        "left_hand_data = {\n",
        "    'labels': left_labels,\n",
        "    'kmeans': left_kmeans,\n",
        "    'latents': left_latents\n",
        "}\n",
        "\n",
        "right_hand_data = {\n",
        "    'labels': right_labels,\n",
        "    'kmeans': right_kmeans,\n",
        "    'latents': right_latents\n",
        "}\n",
        "\n",
        "# Save to files\n",
        "with open(\"/content/pose_clustersnew.pkl\", \"wb\") as f:\n",
        "    pickle.dump(pose_data, f)\n",
        "\n",
        "with open(\"/content/left_hand_clustersnew.pkl\", \"wb\") as f:\n",
        "    pickle.dump(left_hand_data, f)\n",
        "\n",
        "with open(\"/content/right_hand_clustersnew.pkl\", \"wb\") as f:\n",
        "    pickle.dump(right_hand_data, f)\n"
      ],
      "metadata": {
        "id": "vAy1DUAC7BLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "frame_keys = list(all_data.keys())\n",
        "\n",
        "df_clusters = pd.DataFrame({\n",
        "    \"frame\": frame_keys,\n",
        "    \"pose_cluster\": pose_labels,\n",
        "    \"left_cluster\": left_labels,\n",
        "    \"right_cluster\": right_labels\n",
        "})\n",
        "\n",
        "df_clusters.to_csv(\"/content/drive/MyDrive/keypoint_cluster_labelsnew.csv\", index=False)\n",
        "print(\"✅ Saved clustering results.\")\n"
      ],
      "metadata": {
        "id": "JuMcoLt-5Ns3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"Pose Cluster Distribution:\", Counter(pose_labels))\n",
        "print(\"Left Hand Cluster Distribution:\", Counter(left_labels))\n",
        "print(\"Right Hand Cluster Distribution:\", Counter(right_labels))\n"
      ],
      "metadata": {
        "id": "eC8j5mQV5Nqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Cluster centers\n",
        "print(\"Pose Cluster Centers:\\n\", pose_kmeans.cluster_centers_)\n",
        "print(\"\\nLeft Hand Cluster Centers:\\n\", left_kmeans.cluster_centers_)\n",
        "print(\"\\nRight Hand Cluster Centers:\\n\", right_kmeans.cluster_centers_)\n",
        "\n",
        "# Frame counts per cluster\n",
        "print(\"\\nPose Cluster Distribution:\", np.bincount(pose_labels))\n",
        "print(\"Left Hand Cluster Distribution:\", np.bincount(left_labels))\n",
        "print(\"Right Hand Cluster Distribution:\", np.bincount(right_labels))\n"
      ],
      "metadata": {
        "id": "SAMpqA_s5NoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Latent shape (pose):\", pose_latents.shape)\n",
        "print(\"Latent shape (left):\", left_latents.shape)\n",
        "print(\"Latent shape (right):\", right_latents.shape)\n",
        "\n",
        "print(\"Total pose frames:\", len(pose_labels))\n",
        "print(\"Total left hand frames:\", len(left_labels))\n",
        "print(\"Total right hand frames:\", len(right_labels))\n"
      ],
      "metadata": {
        "id": "5molkSI16OGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm import tqdm\n",
        "\n",
        "def plot_elbow(latents, max_k=100, title=''):\n",
        "    inertias = []\n",
        "    k_values = range(1, max_k + 1)\n",
        "\n",
        "    print(f\"Computing KMeans for K=1 to K={max_k}...\")\n",
        "    for k in tqdm(k_values, desc=\"Fitting KMeans\"):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        kmeans.fit(latents)\n",
        "        inertias.append(kmeans.inertia_)\n",
        "\n",
        "    # Print the inertia values\n",
        "    for k, inertia in zip(k_values, inertias):\n",
        "        print(f'K={k}: Inertia={inertia}')\n",
        "\n",
        "    # Plot the elbow curve\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(k_values, inertias, marker='o')\n",
        "    plt.title(f'Elbow Method for {title}')\n",
        "    plt.xlabel('Number of clusters (K)')\n",
        "    plt.ylabel('Inertia')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Dn9COA5W6OC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_elbow(pose_latents,max_k=50, title='Pose Latents')"
      ],
      "metadata": {
        "id": "hNlUQTza6N_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_elbow(left_latents,max_k=100, title='Left Hand Latents')"
      ],
      "metadata": {
        "id": "VcFQY8Xw6N8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_elbow(right_latents, max_k=100,title='Right Hand Latents')"
      ],
      "metadata": {
        "id": "1Sc_mszK6N5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import torch\n",
        "\n",
        "# === Skeleton Connections ===\n",
        "UPPER_BODY_CONNECTIONS = [\n",
        "    (0, 1), (0, 2), (0, 3), (0, 4),\n",
        "    (3,5), (4,6),\n",
        "    (5,7), (6,8)\n",
        "]\n",
        "HAND_CONNECTIONS = [\n",
        "    (0, 1), (0, 17), (1, 2), (2, 3), (3, 4),\n",
        "    (0, 5), (5, 6), (6, 7), (7, 8),\n",
        "    (5, 9), (9, 10), (10, 11), (11, 12),\n",
        "    (9, 13), (13, 14), (14, 15), (15, 16),\n",
        "    (13, 17), (17, 18), (18, 19), (19, 20)\n",
        "]\n",
        "\n",
        "# === Skeleton Trace Generator ===\n",
        "def create_skeleton_trace(kpts, connections, color, name):\n",
        "    lines = []\n",
        "    for i, j in connections:\n",
        "        if i < len(kpts) and j < len(kpts) and not (np.isnan(kpts[i]).any() or np.isnan(kpts[j]).any()):\n",
        "            lines.append(go.Scatter3d(\n",
        "                x=[kpts[i, 0], kpts[j, 0]],\n",
        "                y=[kpts[i, 1], kpts[j, 1]],\n",
        "                z=[kpts[i, 2], kpts[j, 2]],\n",
        "                mode='lines',\n",
        "                line=dict(color=color, width=4),\n",
        "                showlegend=False\n",
        "            ))\n",
        "    valid = ~np.isnan(kpts).any(axis=1)\n",
        "    joints = go.Scatter3d(\n",
        "        x=kpts[valid, 0],\n",
        "        y=kpts[valid, 1],\n",
        "        z=kpts[valid, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=4, color=color),\n",
        "        name=f\"{name} joints\",\n",
        "        showlegend=False\n",
        "    )\n",
        "    return lines + [joints]\n",
        "\n",
        "# === One-Plot-Per-Cluster ===\n",
        "def plot_cluster_centers_separately(cluster_centers, decoder, n_joints, title, connections):\n",
        "    for i, center in enumerate(cluster_centers):\n",
        "        latent_tensor = torch.tensor(center, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            recon = decoder(latent_tensor).cpu().numpy().squeeze()\n",
        "\n",
        "        kpts = recon.reshape((n_joints, 3))\n",
        "        traces = create_skeleton_trace(kpts, connections, color=\"blue\", name=f\"{title} {i}\")\n",
        "\n",
        "        fig = go.Figure(data=traces)\n",
        "        fig.update_layout(\n",
        "            title=f\"{title} Skeleton from Cluster Center {i}\",\n",
        "            scene=dict(\n",
        "                xaxis_title='X', yaxis_title='Y', zaxis_title='Z',\n",
        "                aspectmode='data'\n",
        "            ),\n",
        "            height=500,\n",
        "            width=600,\n",
        "            showlegend=False\n",
        "        )\n",
        "        fig.show()\n"
      ],
      "metadata": {
        "id": "H8gmgKc86N3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show 5 clusters, one per figure\n",
        "num_clusters = 20\n",
        "\n",
        "plot_cluster_centers_separately(pose_kmeans.cluster_centers_[:num_clusters], pose_ae.decoder, 9, \"Pose\", UPPER_BODY_CONNECTIONS)\n",
        "# plot_cluster_centers_separately(left_kmeans.cluster_centers_[:num_clusters], left_ae.decoder, 21, \"Left Hand\", HAND_CONNECTIONS)\n",
        "# plot_cluster_centers_separately(right_kmeans.cluster_centers_[:num_clusters], right_ae.decoder, 21, \"Right Hand\", HAND_CONNECTIONS)\n"
      ],
      "metadata": {
        "id": "ElbJECek6N0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cluster_centers_separately(left_kmeans.cluster_centers_[:num_clusters], left_ae.decoder, 21, \"Left Hand\", HAND_CONNECTIONS)"
      ],
      "metadata": {
        "id": "ddnH26Sk6Nxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cluster_centers_separately(right_kmeans.cluster_centers_[:num_clusters], right_ae.decoder, 21, \"Right Hand\", HAND_CONNECTIONS)\n"
      ],
      "metadata": {
        "id": "uqZ2b8Of6Nu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def plot_cluster_skeletons_2d(\n",
        "    cluster_id,\n",
        "    latents,\n",
        "    labels,\n",
        "    decoder,\n",
        "    n_joints,\n",
        "    connections,\n",
        "    max_members=200000,\n",
        "    title=\"Skeleton Cluster\"\n",
        "):\n",
        "    # Select members of the cluster\n",
        "    indices = np.where(labels == cluster_id)[0][:max_members]\n",
        "\n",
        "    # Assign consistent color per connection\n",
        "    connection_colors = {}\n",
        "    for idx, conn in enumerate(connections):\n",
        "        connection_colors[conn] = plt.cm.tab20(idx % 20)  # up to 20 colors\n",
        "\n",
        "    # Decode all cluster members\n",
        "    all_kpts = []\n",
        "    for idx in indices:\n",
        "        latent = torch.tensor(latents[idx]).unsqueeze(0).float()\n",
        "        with torch.no_grad():\n",
        "            decoded = decoder(latent).cpu().numpy().squeeze()\n",
        "\n",
        "        if decoded.shape[0] == n_joints * 3:\n",
        "            kpts = decoded.reshape(n_joints, 3)[:, :2]  # Only X, Y\n",
        "        elif decoded.shape[0] == n_joints * 2:\n",
        "            kpts = decoded.reshape(n_joints, 2)\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected shape: {decoded.shape}\")\n",
        "\n",
        "        all_kpts.append(kpts)\n",
        "\n",
        "    all_kpts = np.array(all_kpts)  # shape: (N, n_joints, 2)\n",
        "    cluster_center = np.mean(all_kpts, axis=0)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    ax = plt.gca()\n",
        "    ax.set_title(f\"{title} (Cluster {cluster_id}, {len(indices)} members)\", fontsize=12)\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "    # Plot each skeleton\n",
        "    for kpts in all_kpts:\n",
        "        for i, j in connections:\n",
        "            if i < n_joints and j < n_joints:\n",
        "                x = [kpts[i, 0], kpts[j, 0]]\n",
        "                y = [kpts[i, 1], kpts[j, 1]]\n",
        "                ax.plot(x, y, color=connection_colors[(i, j)], alpha=0.1, linewidth=1)\n",
        "        ax.scatter(kpts[:, 0], kpts[:, 1], color=\"black\", s=2, alpha=0.1)  # draw keypoints\n",
        "\n",
        "    # Plot cluster center (bold)\n",
        "    for i, j in connections:\n",
        "        if i < n_joints and j < n_joints:\n",
        "            x = [cluster_center[i, 0], cluster_center[j, 0]]\n",
        "            y = [cluster_center[i, 1], cluster_center[j, 1]]\n",
        "            ax.plot(x, y, color=connection_colors[(i, j)], linewidth=3)\n",
        "    ax.scatter(cluster_center[:, 0], cluster_center[:, 1], color=\"red\", s=20, zorder=10)  # bold keypoints\n",
        "\n",
        "    plt.xlabel(\"X\")\n",
        "    plt.ylabel(\"Y\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "dbRtBabv6NsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_cluster_skeletons_2d_save(\n",
        "    cluster_id,\n",
        "    latents,\n",
        "    labels,\n",
        "    decoder,\n",
        "    n_joints,\n",
        "    connections,\n",
        "    max_members=200000,\n",
        "    title=\"Skeleton Cluster\",\n",
        "    save_path=None\n",
        "):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import torch\n",
        "\n",
        "    indices = np.where(labels == cluster_id)[0][:max_members]\n",
        "\n",
        "    connection_colors = {conn: plt.cm.tab20(idx % 20) for idx, conn in enumerate(connections)}\n",
        "\n",
        "    all_kpts = []\n",
        "    for idx in indices:\n",
        "        latent = torch.tensor(latents[idx]).unsqueeze(0).float()\n",
        "        with torch.no_grad():\n",
        "            decoded = decoder(latent).cpu().numpy().squeeze()\n",
        "\n",
        "        if decoded.shape[0] == n_joints * 3:\n",
        "            kpts = decoded.reshape(n_joints, 3)[:, :2]\n",
        "        elif decoded.shape[0] == n_joints * 2:\n",
        "            kpts = decoded.reshape(n_joints, 2)\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected shape: {decoded.shape}\")\n",
        "\n",
        "        all_kpts.append(kpts)\n",
        "\n",
        "    all_kpts = np.array(all_kpts)\n",
        "    cluster_center = np.mean(all_kpts, axis=0)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    ax = plt.gca()\n",
        "    ax.set_title(f\"{title} (Cluster {cluster_id}, {len(indices)} members)\", fontsize=12)\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "    for kpts in all_kpts:\n",
        "        for i, j in connections:\n",
        "            if i < n_joints and j < n_joints:\n",
        "                x = [kpts[i, 0], kpts[j, 0]]\n",
        "                y = [kpts[i, 1], kpts[j, 1]]\n",
        "                ax.plot(x, y, color=connection_colors[(i, j)], alpha=0.1, linewidth=1)\n",
        "        ax.scatter(kpts[:, 0], kpts[:, 1], color=\"black\", s=2, alpha=0.1)\n",
        "\n",
        "    for i, j in connections:\n",
        "        if i < n_joints and j < n_joints:\n",
        "            x = [cluster_center[i, 0], cluster_center[j, 0]]\n",
        "            y = [cluster_center[i, 1], cluster_center[j, 1]]\n",
        "            ax.plot(x, y, color=connection_colors[(i, j)], linewidth=3)\n",
        "\n",
        "    ax.scatter(cluster_center[:, 0], cluster_center[:, 1], color=\"red\", s=20, zorder=10)\n",
        "\n",
        "    plt.xlabel(\"X\")\n",
        "    plt.ylabel(\"Y\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"✅ Saved: {save_path}\")\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "U5sIUS7D6Npm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_smallest_cluster_ids(labels, num_clusters=5):\n",
        "    cluster_counts = Counter(labels)\n",
        "    smallest_clusters = sorted(cluster_counts.items(), key=lambda x: x[1])[:num_clusters]\n",
        "    print(\" Smallest clusters (ID, size):\", smallest_clusters)\n",
        "    return [cid for cid, _ in smallest_clusters]\n",
        "\n",
        "smallest_cluster_ids = get_smallest_cluster_ids(pose_labels, num_clusters=50)\n"
      ],
      "metadata": {
        "id": "79Z7JBpW6Nm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smallest_cluster_ids_left = get_smallest_cluster_ids(left_labels, num_clusters=100)"
      ],
      "metadata": {
        "id": "fJd_ack26Nkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smallest_cluster_ids_right = get_smallest_cluster_ids(right_labels, num_clusters=100)"
      ],
      "metadata": {
        "id": "UkccLbW_6Nh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Directory in Google Drive\n",
        "save_dir = \"/content/drive/MyDrive/pose_cluster_plots\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Get smallest clusters (already done)\n",
        "# smallest_cluster_ids = get_smallest_cluster_ids(pose_labels, num_clusters=50)\n",
        "\n",
        "# Plot and save all\n",
        "for cid in smallest_cluster_ids:\n",
        "    filename = f\"pose_cluster_{cid:02d}.png\"\n",
        "    save_path = os.path.join(save_dir, filename)\n",
        "\n",
        "    plot_cluster_skeletons_2d_save(\n",
        "        cluster_id=cid,\n",
        "        latents=pose_latents,\n",
        "        labels=pose_labels,\n",
        "        decoder=pose_ae.decoder,\n",
        "        n_joints=9,\n",
        "        connections=UPPER_BODY_CONNECTIONS,\n",
        "        title=f\"Pose Cluster {cid}\",\n",
        "        save_path=save_path\n",
        "    )\n"
      ],
      "metadata": {
        "id": "nRPlk_F_6Nfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cluster_skeletons_2d(\n",
        "    cluster_id= 30,\n",
        "    latents=pose_latents,\n",
        "    labels=pose_labels,\n",
        "    decoder=pose_ae.decoder,\n",
        "    n_joints=9,\n",
        "    connections=UPPER_BODY_CONNECTIONS,\n",
        "    title=\"Pose Cluster\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "yFWyoj7o6NdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_cluster_skeletons_hand_2d(\n",
        "    cluster_id,\n",
        "    latents,\n",
        "    labels,\n",
        "    decoder,\n",
        "    n_joints,\n",
        "    connections,\n",
        "    max_members=200000,\n",
        "    title=\"Skeleton Cluster\",\n",
        "    projection=\"frontal\"  # options: 'frontal', 'side', 'top'\n",
        "):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import torch\n",
        "\n",
        "    indices = np.where(labels == cluster_id)[0][:max_members]\n",
        "\n",
        "    connection_colors = {}\n",
        "    for idx, conn in enumerate(connections):\n",
        "        connection_colors[conn] = plt.cm.tab20(idx % 20)\n",
        "\n",
        "    all_kpts = []\n",
        "    for idx in indices:\n",
        "        latent = torch.tensor(latents[idx]).unsqueeze(0).float()\n",
        "        with torch.no_grad():\n",
        "            decoded = decoder(latent).cpu().numpy().squeeze()\n",
        "\n",
        "        if decoded.shape[0] == n_joints * 3:\n",
        "            kpts = decoded.reshape(n_joints, 3)\n",
        "        elif decoded.shape[0] == n_joints * 2:\n",
        "            temp = decoded.reshape(n_joints, 2)\n",
        "            kpts = np.concatenate([temp, np.zeros((n_joints, 1))], axis=1)  # pad z=0\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected shape: {decoded.shape}\")\n",
        "        all_kpts.append(kpts)\n",
        "\n",
        "    all_kpts = np.array(all_kpts)  # shape: (N, n_joints, 3)\n",
        "    cluster_center = np.mean(all_kpts, axis=0)\n",
        "\n",
        "    # Projection logic: extract 2D coords from 3D\n",
        "    def project(kpts3d):\n",
        "        if projection == \"frontal\":\n",
        "            # View from front: X (left-right), Y (up-down)\n",
        "            # Flip Y to make Y up\n",
        "            return np.stack([kpts3d[:, 0], -kpts3d[:, 1]], axis=1)\n",
        "        elif projection == \"side\":\n",
        "            return np.stack([kpts3d[:, 2], -kpts3d[:, 1]], axis=1)\n",
        "        elif projection == \"top\":\n",
        "            return np.stack([kpts3d[:, 0], kpts3d[:, 2]], axis=1)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid projection\")\n",
        "\n",
        "    projected_kpts = [project(k) for k in all_kpts]\n",
        "    projected_center = project(cluster_center)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(8, 11))\n",
        "    ax = plt.gca()\n",
        "    ax.set_title(f\"{title} (Cluster {cluster_id}, {len(indices)} members)\", fontsize=12)\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "    # Plot each skeleton\n",
        "    for kpts in projected_kpts:\n",
        "        for i, j in connections:\n",
        "            if i < n_joints and j < n_joints:\n",
        "                x = [kpts[i, 0], kpts[j, 0]]\n",
        "                y = [kpts[i, 1], kpts[j, 1]]\n",
        "                ax.plot(x, y, color=connection_colors[(i, j)], alpha=0.1, linewidth=1)\n",
        "        ax.scatter(kpts[:, 0], kpts[:, 1], color=\"black\", s=2, alpha=0.1)\n",
        "\n",
        "    # Cluster center\n",
        "    for i, j in connections:\n",
        "        if i < n_joints and j < n_joints:\n",
        "            x = [projected_center[i, 0], projected_center[j, 0]]\n",
        "            y = [projected_center[i, 1], projected_center[j, 1]]\n",
        "            ax.plot(x, y, color=connection_colors[(i, j)], linewidth=3)\n",
        "    ax.scatter(projected_center[:, 0], projected_center[:, 1], color=\"red\", s=20, zorder=10)\n",
        "\n",
        "    plt.xlabel(\"X\")\n",
        "    plt.ylabel(\"Y\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "1bcZpwJb6NbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_cluster_skeletons_2d(\n",
        "#     cluster_id=smallest_cluster_ids_left[69],\n",
        "#     latents=left_latents,\n",
        "#     labels=left_labels,\n",
        "#     decoder=left_ae.decoder,\n",
        "#     n_joints=21,\n",
        "#     connections=HAND_CONNECTIONS,\n",
        "#     title=\"Left Hand Cluster\",\n",
        "#     projection=\"frontal\"\n",
        "# )\n",
        "plot_cluster_skeletons_hand_2d(\n",
        "    cluster_id=52,\n",
        "    latents=left_latents,\n",
        "    labels=left_labels,\n",
        "    decoder=left_ae.decoder,\n",
        "    n_joints=21,\n",
        "    connections=HAND_CONNECTIONS,\n",
        "    title=\"Left Hand Cluster\",\n",
        "    projection=\"frontal\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "xV4KonKD6NYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cluster_skeletons_hand_2d(\n",
        "    cluster_id=65,\n",
        "    latents=right_latents,\n",
        "    labels=right_labels,\n",
        "    decoder=right_ae.decoder,\n",
        "    n_joints=21,\n",
        "    connections=HAND_CONNECTIONS,\n",
        "    title=\"Right Hand Cluster\",\n",
        "    projection=\"frontal\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "nFPFIF435Nle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cluster_skeletons_hand_2d(\n",
        "    cluster_id=50,\n",
        "    latents=right_latents,\n",
        "    labels=right_labels,\n",
        "    decoder=right_ae.decoder,\n",
        "    n_joints=21,\n",
        "    connections=HAND_CONNECTIONS,\n",
        "    title=\"Right Hand Cluster\",\n",
        "    projection=\"frontal\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "o1ZjRqod5Nil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "# Define the directory path\n",
        "output_dir = \"/content/drive/MyDrive/GestureClusters/\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save to files\n",
        "with open(os.path.join(output_dir, \"pose_clustersnew.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(pose_data, f)\n",
        "\n",
        "with open(os.path.join(output_dir, \"left_hand_clustersnew.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(left_hand_data, f)\n",
        "\n",
        "with open(os.path.join(output_dir, \"right_hand_clustersnew.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(right_hand_data, f)"
      ],
      "metadata": {
        "id": "6vGCxa7v64ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/GestureClusters/pose_clustersnew.pkl\", \"rb\") as f:\n",
        "    pose_data = pickle.load(f)\n",
        "\n",
        "pose_labels = pose_data['labels']\n",
        "pose_kmeans = pose_data['kmeans']\n",
        "pose_latents = pose_data['latents']\n"
      ],
      "metadata": {
        "id": "CH8dHFtM64Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Load from Drive\n",
        "video_path = \"/content/drive/MyDrive/data_composite_01/data/s0001_f_w000128.mp4\"\n",
        "video_id = os.path.basename(video_path)\n",
        "\n",
        "# Init mediapipe modules\n",
        "mp_pose = mp.solutions.pose.Pose(static_image_mode=False)\n",
        "mp_hands_left = mp.solutions.hands.Hands(static_image_mode=False, max_num_hands=1)\n",
        "mp_hands_right = mp.solutions.hands.Hands(static_image_mode=False, max_num_hands=1)\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "keypoint_dict = {}\n",
        "frame_id = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pose_result = mp_pose.process(image)\n",
        "    left_hand_result = mp_hands_left.process(image)\n",
        "    right_hand_result = mp_hands_right.process(image)\n",
        "\n",
        "    pose_kpts = np.full((25, 3), np.nan)\n",
        "    left_kpts = np.full((21, 3), np.nan)\n",
        "    right_kpts = np.full((21, 3), np.nan)\n",
        "\n",
        "    if pose_result.pose_landmarks:\n",
        "        for i, lm in enumerate(pose_result.pose_landmarks.landmark[:25]):\n",
        "            pose_kpts[i] = [lm.x * video_width, lm.y * video_height, lm.z * video_width]\n",
        "\n",
        "    if left_hand_result.multi_hand_landmarks:\n",
        "        for i, lm in enumerate(left_hand_result.multi_hand_landmarks[0].landmark):\n",
        "            left_kpts[i] = [lm.x * video_width, lm.y * video_height, lm.z * video_width]\n",
        "\n",
        "    if right_hand_result.multi_hand_landmarks:\n",
        "        for i, lm in enumerate(right_hand_result.multi_hand_landmarks[0].landmark):\n",
        "            right_kpts[i] = [lm.x * video_width, lm.y * video_height, lm.z * video_width]\n",
        "\n",
        "    key = f\"{video_id}_{frame_id}\"\n",
        "    keypoint_dict[key] = {\n",
        "        \"keypoints\": np.vstack([pose_kpts, left_kpts, right_kpts]),\n",
        "        \"video_width\": video_width,\n",
        "        \"video_height\": video_height\n",
        "    }\n",
        "    frame_id += 1\n",
        "\n",
        "cap.release()\n"
      ],
      "metadata": {
        "id": "-aCU4oyR64XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_align_skeleton(pose, left_hand, right_hand, video_width, video_height):\n",
        "    def normalize_kpts(kpts, part):\n",
        "        if np.isnan(kpts).any():\n",
        "            return kpts\n",
        "\n",
        "        if part == 'pose':\n",
        "            shoulder_l, shoulder_r = kpts[11], kpts[12]\n",
        "            center = (shoulder_l + shoulder_r) / 2.0\n",
        "            kpts_centered = kpts - center\n",
        "            scale = np.linalg.norm(shoulder_l - shoulder_r) * 6.0\n",
        "            scale = scale if scale > 0 else 1.0\n",
        "            kpts_scaled = kpts_centered / scale\n",
        "            direction = kpts[0] - center\n",
        "        else:\n",
        "            center = kpts[0]\n",
        "            kpts_centered = kpts - center\n",
        "            chain = [0, 9, 10, 11, 12]\n",
        "            scale = sum(np.linalg.norm(kpts[chain[i+1]] - kpts[chain[i]]) for i in range(len(chain)-1))\n",
        "            scale = scale if scale > 0 else 1.0\n",
        "            kpts_scaled = kpts_centered / scale\n",
        "            direction = kpts[12] - kpts[0]\n",
        "\n",
        "        direction /= np.linalg.norm(direction) + 1e-6\n",
        "        target = np.array([0, 1, 0])\n",
        "        v = np.cross(direction, target)\n",
        "        s = np.linalg.norm(v)\n",
        "        c = np.dot(direction, target)\n",
        "\n",
        "        if s == 0:\n",
        "            R = np.eye(3)\n",
        "        else:\n",
        "            vx = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])\n",
        "            R = np.eye(3) + vx + (vx @ vx) * ((1 - c) / (s ** 2))\n",
        "\n",
        "        return kpts_scaled @ R.T\n",
        "\n",
        "    pose_new = normalize_kpts(pose, 'pose') if pose is not None else None\n",
        "    left_new = normalize_kpts(left_hand, 'hand') if left_hand is not None else None\n",
        "    right_new = normalize_kpts(right_hand, 'hand') if right_hand is not None else None\n",
        "    return pose_new, left_new, right_new\n"
      ],
      "metadata": {
        "id": "v9uKDWrA64Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in tqdm(keypoint_dict):\n",
        "    entry = keypoint_dict[k]\n",
        "    pose = entry[\"keypoints\"][:25]\n",
        "    left = entry[\"keypoints\"][25:46]\n",
        "    right = entry[\"keypoints\"][46:]\n",
        "    vw = entry[\"video_width\"]\n",
        "    vh = entry[\"video_height\"]\n",
        "\n",
        "    pose_new, left_new, right_new = normalize_and_align_skeleton(pose, left, right, vw, vh)\n",
        "    keypoint_dict[k][\"keypoints\"] = np.vstack([pose_new, left_new, right_new])\n"
      ],
      "metadata": {
        "id": "vcLE405A64Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/processed_data/keypoint_dict_normalizednew.pkl\", \"wb\") as f:\n",
        "    pickle.dump(keypoint_dict, f)\n",
        "\n",
        "print(\"✅ Saved normalized keypoints to Drive.\")\n"
      ],
      "metadata": {
        "id": "sw04dP9b64NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Autoencoderpose and Autoencoder class first if not already defined\n",
        "# Assuming you have them defined as `Autoencoderpose(input_size)` and `Autoencoder(input_size)`\n",
        "\n",
        "ae_pose = Autoencoderpose(27)  # 9 keypoints × 3 coords\n",
        "ae_pose.load_state_dict(torch.load(\"/content/drive/MyDrive/autoencoder_models/pose_aesest.pth\", map_location=\"cpu\"))\n",
        "ae_pose.eval()\n",
        "\n",
        "ae_left = Autoencoder(63)  # 21 × 3\n",
        "ae_left.load_state_dict(torch.load(\"/content/drive/MyDrive/autoencoder_models/left_hand_ae4.pth\", map_location=\"cpu\"))\n",
        "ae_left.eval()\n",
        "\n",
        "ae_right = Autoencoder(63)  # 21 × 3\n",
        "ae_right.load_state_dict(torch.load(\"/content/drive/MyDrive/autoencoder_models/right_hand_ae4.pth\", map_location=\"cpu\"))\n",
        "ae_right.eval()\n"
      ],
      "metadata": {
        "id": "uuVBsY6N64Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load normalized keypoints\n",
        "with open(\"/content/drive/MyDrive/processed_data/keypoint_dict_normalizednew.pkl\", \"rb\") as f:\n",
        "    keypoint_dict = pickle.load(f)\n",
        "\n",
        "# Correct upper body indices\n",
        "upper_body_idx = [0, 2, 5, 11, 12, 13, 14, 15, 16]\n",
        "\n",
        "pose_latents = {}\n",
        "left_latents = {}\n",
        "right_latents = {}\n",
        "\n",
        "for k in tqdm(keypoint_dict):\n",
        "    kpts = keypoint_dict[k][\"keypoints\"]  # shape: (67, 3)\n",
        "\n",
        "    # Extract parts\n",
        "    pose = kpts[upper_body_idx].flatten()     # 9×3 = 27\n",
        "    left = kpts[25:46].flatten()              # 21×3 = 63\n",
        "    right = kpts[46:67].flatten()             # 21×3 = 63\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    pose_tensor = torch.tensor(pose, dtype=torch.float32).unsqueeze(0)\n",
        "    left_tensor = torch.tensor(left, dtype=torch.float32).unsqueeze(0)\n",
        "    right_tensor = torch.tensor(right, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    # Encode\n",
        "    with torch.no_grad():\n",
        "        pose_latents[k] = ae_pose.encoder(pose_tensor).numpy().flatten()\n",
        "        left_latents[k] = ae_left.encoder(left_tensor).numpy().flatten()\n",
        "        right_latents[k] = ae_right.encoder(right_tensor).numpy().flatten()\n"
      ],
      "metadata": {
        "id": "Hq1zP-Ov64IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save\n",
        "with open(\"/content/drive/MyDrive/processed_data/pose_latentsnew.pkl\", \"wb\") as f:\n",
        "    pickle.dump(pose_latents, f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/processed_data/left_latentsnew.pkl\", \"wb\") as f:\n",
        "    pickle.dump(left_latents, f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/processed_data/right_latentsnew.pkl\", \"wb\") as f:\n",
        "    pickle.dump(right_latents, f)\n",
        "\n",
        "print(\"✅ Latent vectors saved.\")\n"
      ],
      "metadata": {
        "id": "PmetCDKw64FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "# Load latent vectors\n",
        "with open(\"/content/drive/MyDrive/processed_data/pose_latentsnew.pkl\", \"rb\") as f:\n",
        "    pose_latents = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/processed_data/left_latentsnew.pkl\", \"rb\") as f:\n",
        "    left_latents = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/processed_data/right_latentsnew.pkl\", \"rb\") as f:\n",
        "    right_latents = pickle.load(f)\n",
        "\n",
        "# Load cluster models\n",
        "with open(\"/content/drive/MyDrive/GestureClusters/pose_clustersnew.pkl\", \"rb\") as f:\n",
        "    pose_kmeans = pickle.load(f)[\"kmeans\"]\n",
        "\n",
        "with open(\"/content/drive/MyDrive/GestureClusters/left_hand_clustersnew.pkl\", \"rb\") as f:\n",
        "    left_kmeans = pickle.load(f)[\"kmeans\"]\n",
        "\n",
        "with open(\"/content/drive/MyDrive/GestureClusters/right_hand_clustersnew.pkl\", \"rb\") as f:\n",
        "    right_kmeans = pickle.load(f)[\"kmeans\"]\n"
      ],
      "metadata": {
        "id": "XzzX7C_R64BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pose_cluster_map = {}\n",
        "left_cluster_map = {}\n",
        "right_cluster_map = {}\n",
        "\n",
        "for key in pose_latents:\n",
        "    pose_vector = pose_latents[key].reshape(1, -1)\n",
        "    left_vector = left_latents[key].reshape(1, -1)\n",
        "    right_vector = right_latents[key].reshape(1, -1)\n",
        "\n",
        "    # Check for NaNs\n",
        "    if np.isnan(pose_vector).any() or np.isnan(left_vector).any() or np.isnan(right_vector).any():\n",
        "        print(f\"⚠️ Skipping {key} due to NaNs\")\n",
        "        continue\n",
        "\n",
        "    # Predict clusters\n",
        "    pose_cluster_map[key] = pose_kmeans.predict(pose_vector)[0]\n",
        "    left_cluster_map[key] = left_kmeans.predict(left_vector)[0]\n",
        "    right_cluster_map[key] = right_kmeans.predict(right_vector)[0]\n"
      ],
      "metadata": {
        "id": "bUvj2tDn63-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/processed_data/cluster_ids_pose.pkl\", \"wb\") as f:\n",
        "    pickle.dump(pose_cluster_map, f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/processed_data/cluster_ids_left.pkl\", \"wb\") as f:\n",
        "    pickle.dump(left_cluster_map, f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/processed_data/cluster_ids_right.pkl\", \"wb\") as f:\n",
        "    pickle.dump(right_cluster_map, f)\n",
        "\n",
        "print(\"✅ Saved cluster assignments.\")\n"
      ],
      "metadata": {
        "id": "SbkcVDTn637m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Define connections for the small 9-joint upper body pose\n",
        "POSE_CONNECTIONS = [\n",
        "    (0, 1), (0, 2),         # Nose to eyes\n",
        "    (0, 3), (0, 4),         # Nose to shoulders ❗\n",
        "    (3, 5), (5, 7),         # Left: shoulder → elbow → wrist\n",
        "    (4, 6), (6, 8)          # Right: shoulder → elbow → wrist\n",
        "]\n",
        "\n",
        "# Standard MediaPipe hand connections\n",
        "HAND_CONNECTIONS = [\n",
        "    (0, 1), (1, 2), (2, 3), (3, 4),\n",
        "    (0, 5), (5, 6), (6, 7), (7, 8),\n",
        "    (0, 9), (9,10), (10,11), (11,12),\n",
        "    (0,13), (13,14), (14,15), (15,16),\n",
        "    (0,17), (17,18), (18,19), (19,20)\n",
        "]\n",
        "\n",
        "def decode_latents(pose_cluster, left_cluster, right_cluster):\n",
        "    # Latent vectors\n",
        "    pose_latent = pose_kmeans.cluster_centers_[pose_cluster]\n",
        "    left_latent = left_kmeans.cluster_centers_[left_cluster]\n",
        "    right_latent = right_kmeans.cluster_centers_[right_cluster]\n",
        "\n",
        "    # Decode to keypoints\n",
        "    decoded_pose = pose_decoder(torch.tensor(pose_latent).float().unsqueeze(0)).detach().numpy().reshape(9, 3)\n",
        "    decoded_left = left_decoder(torch.tensor(left_latent).float().unsqueeze(0)).detach().numpy().reshape(21, 3)\n",
        "    decoded_right = right_decoder(torch.tensor(right_latent).float().unsqueeze(0)).detach().numpy().reshape(21, 3)\n",
        "\n",
        "    return decoded_pose, decoded_left, decoded_right\n"
      ],
      "metadata": {
        "id": "_35O12gq635J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def plot_combined_clusters(pose_kp, left_kp, right_kp, frame_id, save_path=None):\n",
        "#     # Place hands at wrist joints from pose\n",
        "#     left_wrist = pose_kp[7]   # left wrist in 9-joint set\n",
        "#     right_wrist = pose_kp[8]  # right wrist in 9-joint set\n",
        "\n",
        "#     # Align and scale hands\n",
        "#     hand_scale = 0.2\n",
        "#     left_aligned = left_kp * hand_scale + left_wrist\n",
        "#     right_aligned = right_kp * hand_scale + right_wrist\n",
        "\n",
        "#     # Begin plotting\n",
        "#     fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "#     # Pose connections\n",
        "#     for i, j in POSE_CONNECTIONS:\n",
        "#         ax.plot([pose_kp[i, 0], pose_kp[j, 0]],\n",
        "#                 [pose_kp[i, 1], pose_kp[j, 1]], 'b-')\n",
        "\n",
        "#     # Left hand\n",
        "#     for i, j in HAND_CONNECTIONS:\n",
        "#         ax.plot([left_aligned[i, 0], left_aligned[j, 0]],\n",
        "#                 [left_aligned[i, 1], left_aligned[j, 1]], 'g-')\n",
        "\n",
        "#     # Right hand\n",
        "#     for i, j in HAND_CONNECTIONS:\n",
        "#         ax.plot([right_aligned[i, 0], right_aligned[j, 0]],\n",
        "#                 [right_aligned[i, 1], right_aligned[j, 1]], 'r-')\n",
        "\n",
        "#     ax.set_title(f\"Cluster Skeletons for Frame {frame_id}\")\n",
        "#     ax.set_aspect('equal')\n",
        "#     ax.invert_yaxis()\n",
        "#     ax.grid(True)\n",
        "#     ax.legend(['Pose', 'Left Hand', 'Right Hand'])\n",
        "\n",
        "#     if save_path:\n",
        "#         plt.savefig(save_path)\n",
        "#         print(f\"✅ Saved: {save_path}\")\n",
        "#     plt.close()\n",
        "def plot_skeleton(pose_kps, left_kps, right_kps, frame_id, save_path=None):\n",
        "    # 🔄 Flip Y of pose (not hands) to fix upside-down skeleton\n",
        "    pose_kps[:, 1] *= -1\n",
        "\n",
        "    # Align hands to wrists\n",
        "    left_wrist = pose_kps[7]\n",
        "    right_wrist = pose_kps[8]\n",
        "\n",
        "    hand_scale = 0.2\n",
        "    left_hand_shifted = left_kps * hand_scale + left_wrist\n",
        "    right_hand_shifted = right_kps * hand_scale + right_wrist\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "    # Plot Pose\n",
        "    for i, j in POSE_CONNECTIONS:\n",
        "        ax.plot([pose_kps[i, 0], pose_kps[j, 0]], [pose_kps[i, 1], pose_kps[j, 1]], 'b-')\n",
        "\n",
        "    # Plot Left Hand\n",
        "    for i, j in HAND_CONNECTIONS:\n",
        "        ax.plot([left_hand_shifted[i, 0], left_hand_shifted[j, 0]], [left_hand_shifted[i, 1], left_hand_shifted[j, 1]], 'g-')\n",
        "\n",
        "    # Plot Right Hand\n",
        "    for i, j in HAND_CONNECTIONS:\n",
        "        ax.plot([right_hand_shifted[i, 0], right_hand_shifted[j, 0]], [right_hand_shifted[i, 1], right_hand_shifted[j, 1]], 'r-')\n",
        "\n",
        "    ax.set_title(f\"Frame {frame_id}\")\n",
        "    ax.set_aspect('equal')\n",
        "    ax.invert_yaxis()  # Optional: Keep if image coordinates preferred\n",
        "    ax.grid(True)\n",
        "    ax.legend(['Pose', 'Left Hand', 'Right Hand'])\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"✅ Saved: {save_path}\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "N3h6Y36v632o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load decoders\n",
        "pose_decoder = ae_pose.decoder\n",
        "left_decoder = ae_left.decoder\n",
        "right_decoder = ae_right.decoder\n",
        "\n",
        "pose_decoder.eval()\n",
        "left_decoder.eval()\n",
        "right_decoder.eval()\n"
      ],
      "metadata": {
        "id": "Ru4X5Xwz630G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = \"/content/drive/MyDrive/skeleton_plotsc\"\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "video_id = \"s0001_f_w000128.mp4\"\n",
        "frame_keys = sorted(\n",
        "    [k for k in pose_latents if k.startswith(video_id)],\n",
        "    key=lambda x: int(x.split(\"_\")[-1])\n",
        ")\n",
        "\n",
        "for full_key in frame_keys:\n",
        "    frame_id = int(full_key.split(\"_\")[-1])\n",
        "    pose_vector = pose_latents[full_key].reshape(1, -1)\n",
        "    left_vector = left_latents[full_key].reshape(1, -1)\n",
        "    right_vector = right_latents[full_key].reshape(1, -1)\n",
        "\n",
        "    if np.isnan(pose_vector).any() or np.isnan(left_vector).any() or np.isnan(right_vector).any():\n",
        "        print(f\"⚠️ Skipping {full_key} due to NaNs\")\n",
        "        continue\n",
        "\n",
        "    pose_cid = pose_kmeans.predict(pose_vector)[0]\n",
        "    left_cid = left_kmeans.predict(left_vector)[0]\n",
        "    right_cid = right_kmeans.predict(right_vector)[0]\n",
        "\n",
        "    pose_kps, left_kps, right_kps = decode_latents(pose_cid, left_cid, right_cid)\n",
        "\n",
        "    save_path = f\"{save_directory}/{video_id}_frame_{frame_id:04d}.png\"\n",
        "    plot_skeleton(pose_kps, left_kps, right_kps, frame_id, save_path)\n"
      ],
      "metadata": {
        "id": "SohJnFKV63xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_skeleton_new(pose_kps, left_kps, right_kps, frame_id, save_path=None):\n",
        "    # 🔄 Flip Y of pose (not hands) to fix upside-down skeleton\n",
        "    pose_kps[:, 1] *= -1\n",
        "\n",
        "    # Align hands to wrists\n",
        "    left_wrist = pose_kps[7]\n",
        "    right_wrist = pose_kps[8]\n",
        "\n",
        "    def align_hand_to_wrist(hand_kps, wrist_pos, flip=True):\n",
        "    # Vector from wrist to middle fingertip (approx hand direction)\n",
        "          direction = hand_kps[12] - hand_kps[0]\n",
        "          direction = direction / (np.linalg.norm(direction) + 1e-6)\n",
        "\n",
        "          # Target direction (Y-up)\n",
        "          target = np.array([0, 1, 0])\n",
        "\n",
        "          # Rotation matrix to align direction to target\n",
        "          v = np.cross(direction, target)\n",
        "          s = np.linalg.norm(v)\n",
        "          c = np.dot(direction, target)\n",
        "\n",
        "          if s < 1e-6:\n",
        "              R = np.eye(3)\n",
        "          else:\n",
        "              vx = np.array([\n",
        "                  [0, -v[2], v[1]],\n",
        "                  [v[2], 0, -v[0]],\n",
        "                  [-v[1], v[0], 0]\n",
        "              ])\n",
        "              R = np.eye(3) + vx + (vx @ vx) * ((1 - c) / (s ** 2))\n",
        "\n",
        "          # Rotate and shift\n",
        "          scaled = hand_kps * 0.2\n",
        "          rotated = scaled @ R.T\n",
        "          return rotated + wrist_pos\n",
        "\n",
        "    # Apply this:\n",
        "    left_hand_shifted = align_hand_to_wrist(left_kps, left_wrist)\n",
        "    right_hand_shifted = align_hand_to_wrist(right_kps, right_wrist)\n",
        "\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "    # Plot Pose\n",
        "    for i, j in POSE_CONNECTIONS:\n",
        "        ax.plot([pose_kps[i, 0], pose_kps[j, 0]], [pose_kps[i, 1], pose_kps[j, 1]], 'b-')\n",
        "\n",
        "    # Plot Left Hand\n",
        "    for i, j in HAND_CONNECTIONS:\n",
        "        ax.plot([left_hand_shifted[i, 0], left_hand_shifted[j, 0]], [left_hand_shifted[i, 1], left_hand_shifted[j, 1]], 'g-')\n",
        "\n",
        "    # Plot Right Hand\n",
        "    for i, j in HAND_CONNECTIONS:\n",
        "        ax.plot([right_hand_shifted[i, 0], right_hand_shifted[j, 0]], [right_hand_shifted[i, 1], right_hand_shifted[j, 1]], 'r-')\n",
        "\n",
        "    ax.set_title(f\"Frame {frame_id}\")\n",
        "    ax.set_aspect('equal')\n",
        "    ax.invert_yaxis()  # Optional: Keep if image coordinates preferred\n",
        "    ax.grid(True)\n",
        "    ax.legend(['Pose', 'Left Hand', 'Right Hand'])\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"✅ Saved: {save_path}\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "MkenO7AO720J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = \"/content/drive/MyDrive/skeleton_plotsn\"\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "video_id = \"s0001_f_w000128.mp4\"\n",
        "frame_keys = sorted(\n",
        "    [k for k in pose_latents if k.startswith(video_id)],\n",
        "    key=lambda x: int(x.split(\"_\")[-1])\n",
        ")\n",
        "\n",
        "for full_key in frame_keys:\n",
        "    frame_id = int(full_key.split(\"_\")[-1])\n",
        "    pose_vector = pose_latents[full_key].reshape(1, -1)\n",
        "    left_vector = left_latents[full_key].reshape(1, -1)\n",
        "    right_vector = right_latents[full_key].reshape(1, -1)\n",
        "\n",
        "    if np.isnan(pose_vector).any() or np.isnan(left_vector).any() or np.isnan(right_vector).any():\n",
        "        print(f\"⚠️ Skipping {full_key} due to NaNs\")\n",
        "        continue\n",
        "\n",
        "    pose_cid = pose_kmeans.predict(pose_vector)[0]\n",
        "    left_cid = left_kmeans.predict(left_vector)[0]\n",
        "    right_cid = right_kmeans.predict(right_vector)[0]\n",
        "\n",
        "    pose_kps, left_kps, right_kps = decode_latents(pose_cid, left_cid, right_cid)\n",
        "\n",
        "    save_path = f\"{save_directory}/{video_id}_frame_{frame_id:04d}.png\"\n",
        "    plot_skeleton_new(pose_kps, left_kps, right_kps, frame_id, save_path)\n"
      ],
      "metadata": {
        "id": "Nepmnprc751J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Load the normalized keypoints\n",
        "with open(\"/content/drive/MyDrive/all_transformed_keypoints.pkl\", \"rb\") as f:\n",
        "    all_keypoints = pickle.load(f)\n",
        "\n",
        "# Load trained encoders and cluster models\n",
        "ae_pose = Autoencoderpose(27)\n",
        "ae_pose.load_state_dict(torch.load(\"/content/drive/MyDrive/autoencoder_models/pose_aesest.pth\", map_location='cpu'))\n",
        "ae_pose.eval()\n",
        "\n",
        "ae_left = Autoencoder(63)\n",
        "ae_left.load_state_dict(torch.load(\"/content/drive/MyDrive/autoencoder_models/left_hand_ae4.pth\", map_location='cpu'))\n",
        "ae_left.eval()\n",
        "\n",
        "ae_right = Autoencoder(63)\n",
        "ae_right.load_state_dict(torch.load(\"/content/drive/MyDrive/autoencoder_models/right_hand_ae4.pth\", map_location='cpu'))\n",
        "ae_right.eval()\n",
        "\n",
        "# Load cluster models\n",
        "with open(\"/content/drive/MyDrive/GestureClusters/pose_clustersnew.pkl\", \"rb\") as f:\n",
        "    pose_kmeans = pickle.load(f)[\"kmeans\"]\n",
        "\n",
        "with open(\"/content/drive/MyDrive/GestureClusters/left_hand_clustersnew.pkl\", \"rb\") as f:\n",
        "    left_kmeans = pickle.load(f)[\"kmeans\"]\n",
        "\n",
        "with open(\"/content/drive/MyDrive/GestureClusters/right_hand_clustersnew.pkl\", \"rb\") as f:\n",
        "    right_kmeans = pickle.load(f)[\"kmeans\"]\n",
        "\n",
        "# Define 9 pose indices used for training Autoencoderpose\n",
        "UPPER_BODY_IDX = [0, 2, 5, 11, 12, 13, 14, 15, 16]\n",
        "\n",
        "# Dictionary to store results\n",
        "framewise_cluster_ids = {}\n",
        "\n",
        "# Convert to latent and get cluster ID\n",
        "for key in tqdm(sorted(all_keypoints)):\n",
        "    entry = all_keypoints[key]\n",
        "\n",
        "    try:\n",
        "        pose = entry[\"pose\"][UPPER_BODY_IDX, :3].flatten()      # (9 x 3) = 27\n",
        "        left = entry[\"left hand\"].flatten()                     # (21 x 3) = 63\n",
        "        right = entry[\"right hand\"].flatten()                   # (21 x 3) = 63\n",
        "\n",
        "        pose_tensor = torch.tensor(pose, dtype=torch.float32).unsqueeze(0)\n",
        "        left_tensor = torch.tensor(left, dtype=torch.float32).unsqueeze(0)\n",
        "        right_tensor = torch.tensor(right, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pose_latent = ae_pose.encoder(pose_tensor).numpy()\n",
        "            left_latent = ae_left.encoder(left_tensor).numpy()\n",
        "            right_latent = ae_right.encoder(right_tensor).numpy()\n",
        "\n",
        "        pose_cluster = pose_kmeans.predict(pose_latent)[0]\n",
        "        left_cluster = left_kmeans.predict(left_latent)[0]\n",
        "        right_cluster = right_kmeans.predict(right_latent)[0]\n",
        "\n",
        "        framewise_cluster_ids[key] = {\n",
        "            \"pose_cluster\": pose_cluster,\n",
        "            \"left_cluster\": left_cluster,\n",
        "            \"right_cluster\": right_cluster\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in {key}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Save result\n",
        "with open(\"/content/drive/MyDrive/framewise_cluster_ids.pkl\", \"wb\") as f:\n",
        "    pickle.dump(framewise_cluster_ids, f)\n",
        "\n",
        "print(\"✅ Saved: framewise_cluster_ids.pkl\")\n"
      ],
      "metadata": {
        "id": "1P9rpc7z75yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load your framewise cluster triplets\n",
        "with open(\"/content/drive/MyDrive/framewise_cluster_ids.pkl\", \"rb\") as f:\n",
        "    framewise_clusters = pickle.load(f)\n",
        "\n",
        "# Step 1: Group by video and create observation sequences\n",
        "video_obs = defaultdict(list)\n",
        "\n",
        "for frame_key in sorted(framewise_clusters.keys()):\n",
        "    video_id = \"_\".join(frame_key.split(\"_\")[:-1])\n",
        "    triplet = (\n",
        "        framewise_clusters[frame_key][\"pose_cluster\"],\n",
        "        framewise_clusters[frame_key][\"left_cluster\"],\n",
        "        framewise_clusters[frame_key][\"right_cluster\"]\n",
        "    )\n",
        "    video_obs[video_id].append(triplet)\n",
        "\n",
        "# Step 2: Map each unique triplet to an integer\n",
        "triplet_to_id = {}\n",
        "id_to_triplet = []\n",
        "obs_sequences = []\n",
        "\n",
        "for vid in sorted(video_obs.keys()):\n",
        "    sequence = []\n",
        "    for triplet in video_obs[vid]:\n",
        "        if triplet not in triplet_to_id:\n",
        "            triplet_to_id[triplet] = len(id_to_triplet)\n",
        "            id_to_triplet.append(triplet)\n",
        "        sequence.append(triplet_to_id[triplet])\n",
        "    obs_sequences.append(sequence)\n",
        "\n",
        "# Save the results\n",
        "with open(\"/content/drive/MyDrive/bphmm_obs_sequences.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"obs_sequences\": obs_sequences,\n",
        "        \"triplet_to_id\": triplet_to_id,\n",
        "        \"id_to_triplet\": id_to_triplet\n",
        "    }, f)\n",
        "\n",
        "print(\"✅ Saved: bphmm_obs_sequences.pkl\")\n",
        "print(f\"Total unique observation symbols: {len(id_to_triplet)}\")\n",
        "print(f\"Total videos processed: {len(obs_sequences)}\")\n"
      ],
      "metadata": {
        "id": "0eqCkdjg75vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hidden Markov Model"
      ],
      "metadata": {
        "id": "2mTKW1PX8Vam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hmmlearn\n"
      ],
      "metadata": {
        "id": "7-BgV_ga75su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from hmmlearn import hmm\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "43h8R62X75qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your cluster-ID triplet sequences\n",
        "with open(\"/content/drive/MyDrive/bphmm_obs_sequences_int.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "obs_sequences = data[\"obs_sequences\"]  # list of [int, int, int, ...] per video\n",
        "all_obs = np.concatenate(obs_sequences).reshape(-1, 1)  # hmmlearn expects shape (n_samples, 1)\n",
        "\n",
        "print(f\"✅ Loaded {len(obs_sequences)} sequences\")\n",
        "print(f\"🧩 Total observations: {len(all_obs)}\")\n"
      ],
      "metadata": {
        "id": "L3Y479yV75ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_obs.shape)\n",
        "print(all_obs[:10])\n"
      ],
      "metadata": {
        "id": "pUODfO-q75lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Max obs:\", np.max(all_obs))\n"
      ],
      "metadata": {
        "id": "-qd2loA475iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data[\"id_to_triplet\"]))  # should print something like 9870\n"
      ],
      "metadata": {
        "id": "N34qMzt975fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model states: {model.n_components}\")\n",
        "print(f\"Model features: {model.n_features}\")\n"
      ],
      "metadata": {
        "id": "8S76D8UJ75c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"📊 HMM configured with {model.n_features} observation symbols.\")\n"
      ],
      "metadata": {
        "id": "0H5jdGII75at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "triplet_vectors = np.array(id_to_triplet)  # shape (9870, 3)\n",
        "n_obs_clusters = 512                       # Try 64, 128, or 256\n",
        "\n",
        "kmeans = KMeans(n_clusters=n_obs_clusters, random_state=42).fit(triplet_vectors)\n",
        "\n",
        "# Map old triplet ID to new reduced cluster ID\n",
        "triplet_id_to_new = {i: int(label) for i, label in enumerate(kmeans.labels_)}\n",
        "\n",
        "# Apply to all observation sequences\n",
        "obs_sequences_reclustered = [\n",
        "    [triplet_id_to_new[x] for x in seq]\n",
        "    for seq in obs_sequences\n",
        "]\n",
        "\n",
        "print(f\"✅ Reclustered triplets → {n_obs_clusters} observation symbols.\")\n"
      ],
      "metadata": {
        "id": "HFuIyUfh75YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten and reshape for hmmlearn\n",
        "all_obs = np.concatenate(obs_sequences_reclustered).reshape(-1, 1)\n",
        "print(f\"📐 HMM input shape: {all_obs.shape}\")  # Should be (N, 1)\n"
      ],
      "metadata": {
        "id": "KCO14DMs75Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File from your earlier pipeline\n",
        "with open(\"/content/drive/MyDrive/bphmm_obs_sequences_int.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "obs_sequences = data[\"obs_sequences\"]         # List of videos (each: list of int IDs)\n",
        "id_to_triplet = data[\"id_to_triplet\"]         # 9870 unique (pose, left, right) triplets\n"
      ],
      "metadata": {
        "id": "I1PSCi5a75SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "n_obs_clusters = 512  # or 256, 128, etc.\n",
        "\n",
        "# Each triplet is a 3D vector (pose, left hand, right hand cluster)\n",
        "triplet_vectors = np.array(id_to_triplet)  # shape (9870, 3)\n",
        "\n",
        "# Cluster them\n",
        "kmeans = KMeans(n_clusters=n_obs_clusters, random_state=42)\n",
        "kmeans.fit(triplet_vectors)\n",
        "\n",
        "# Map original triplet ID to new reduced ID\n",
        "triplet_id_to_new = {i: int(label) for i, label in enumerate(kmeans.labels_)}\n",
        "\n",
        "# Apply to all sequences\n",
        "obs_sequences_reclustered = [\n",
        "    [triplet_id_to_new[x] for x in seq] for seq in obs_sequences\n",
        "]\n",
        "\n",
        "print(f\"✅ Reclustered: {len(obs_sequences)} videos → {n_obs_clusters} obs symbols\")\n"
      ],
      "metadata": {
        "id": "qH2YB9g475Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reclustered_data = {\n",
        "    \"obs_sequences\": obs_sequences_reclustered,\n",
        "    \"n_obs_clusters\": n_obs_clusters,\n",
        "    \"triplet_id_to_new\": triplet_id_to_new\n",
        "}\n",
        "\n",
        "with open(\"/content/drive/MyDrive/bphmm_obs_sequences_int_reclustered.pkl\", \"wb\") as f:\n",
        "    pickle.dump(reclustered_data, f)\n",
        "\n",
        "print(\"💾 Reclustered observation sequences saved.\")\n"
      ],
      "metadata": {
        "id": "dOOD0mvI75NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"/content/drive/MyDrive/bphmm_obs_sequences_int_reclustered.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "obs_sequences_reclustered = data[\"obs_sequences\"]\n",
        "V = data[\"n_obs_clusters\"]\n",
        "\n",
        "print(f\"✅ Loaded {len(obs_sequences_reclustered)} sequences with {V} observation symbols.\")\n"
      ],
      "metadata": {
        "id": "gilAu87W75KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert each sequence into list of ints (required by pomegranate)\n",
        "sequences = [[int(x) for x in seq] for seq in obs_sequences_reclustered]\n"
      ],
      "metadata": {
        "id": "LuFpBdO89fYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hmmlearn\n"
      ],
      "metadata": {
        "id": "3TKkn8rz9fVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hmmlearn import hmm\n",
        "import numpy as np\n",
        "import os\n",
        "import joblib\n",
        "import pickle\n",
        "from tqdm import trange\n",
        "\n",
        "# === Load reclustered observations ===\n",
        "with open(\"/content/drive/MyDrive/bphmm_obs_sequences_int_reclustered.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "obs_sequences = data[\"obs_sequences\"]  # List of list of obs IDs\n",
        "V = data[\"n_obs_clusters\"]             # Should be 512\n",
        "\n",
        "# Flatten for hmmlearn\n",
        "all_obs = np.concatenate(obs_sequences).reshape(-1, 1)\n",
        "print(f\"📐 all_obs shape: {all_obs.shape}\")\n"
      ],
      "metadata": {
        "id": "IRsr1CfY9fSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hmmlearn.hmm import CategoricalHMM\n",
        "from hmmlearn.hmm import CategoricalHMM\n",
        "from tqdm import trange\n",
        "import joblib\n",
        "import os\n",
        "import numpy as np\n",
        "from hmmlearn import hmm\n",
        "import pickle\n",
        "\n",
        "# === HMM Setup ===\n",
        "n_states = 270\n",
        "n_epochs = 60\n",
        "\n",
        "model = CategoricalHMM(\n",
        "    n_components=n_states,\n",
        "    n_iter=1,             # One EM iteration per .fit()\n",
        "    verbose=False,\n",
        "    tol=1e-2,\n",
        "    init_params=\"e\"       # Only initialize emissions initially\n",
        ")\n",
        "model.n_features = V     # Must set before training!\n",
        "\n",
        "# === Directory to Save Models ===\n",
        "save_dir = \"/content/drive/MyDrive/hmm_models_categorical\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "log_likelihoods = []\n",
        "\n",
        "print(f\"🚀 Training HMM with {n_states} states and {V} observation symbols...\")\n",
        "\n",
        "for epoch in trange(n_epochs, desc=\"Training HMM Epochs\"):\n",
        "    model.fit(all_obs)  # Run one EM step\n",
        "    log_prob = model.score(all_obs)\n",
        "    log_likelihoods.append(log_prob)\n",
        "\n",
        "    print(f\"📈 Epoch {epoch+1}/{n_epochs} — Log-likelihood: {log_prob:.4f}\")\n",
        "\n",
        "    # Save model\n",
        "    model_path = os.path.join(save_dir, f\"hmm_epoch_{epoch+1}.pkl\")\n",
        "    joblib.dump(model, model_path)\n",
        "    print(f\"💾 Saved model to: {model_path}\")\n",
        "\n",
        "print(\"✅ All epochs complete and saved.\")\n",
        "import pickle\n",
        "import joblib\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from hmmlearn import hmm\n",
        "\n",
        "# === Load trained model ===\n",
        "model_path = \"/content/drive/MyDrive/hmm_models_categorical/hmm_epoch_60.pkl\"\n",
        "model = joblib.load(model_path)\n",
        "\n",
        "# === Load clustered observation sequences ===\n",
        "with open(\"/content/drive/MyDrive/bphmm_obs_sequences_int_reclustered.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "obs_sequences = data[\"obs_sequences\"]\n",
        "\n",
        "# === Decode using Viterbi ===\n",
        "latent_state_sequences = []\n",
        "\n",
        "print(\"🔍 Running Viterbi decoding for all videos...\")\n",
        "for seq in tqdm(obs_sequences, desc=\"Decoding\"):\n",
        "    X = np.array(seq).reshape(-1, 1)\n",
        "    logprob, state_seq = model.decode(X, algorithm=\"viterbi\")\n",
        "    latent_state_sequences.append(state_seq)\n",
        "\n",
        "print(\"✅ Predicted latent state sequences for all videos.\")\n",
        "\n",
        "# === Save latent states ===\n",
        "save_path = \"/content/drive/MyDrive/hmm_viterbi_latent_states.pkl\"\n",
        "with open(save_path, \"wb\") as f:\n",
        "    pickle.dump(latent_state_sequences, f)\n",
        "\n",
        "print(f\"💾 Saved Viterbi latent state sequences to: {save_path}\")\n"
      ],
      "metadata": {
        "id": "a02MBnVZ9fPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === Load predicted latent states ===\n",
        "with open(\"/content/drive/MyDrive/hmm_viterbi_latent_states.pkl\", \"rb\") as f:\n",
        "    latent_state_sequences = pickle.load(f)\n",
        "\n",
        "def plot_latent_states(z_sequences, num_videos=5):\n",
        "    \"\"\"\n",
        "    Visualize latent state sequences for a few videos.\n",
        "    \"\"\"\n",
        "    for i in range(min(num_videos, len(z_sequences))):\n",
        "        plt.figure(figsize=(10, 1.2))\n",
        "        plt.title(f\"Video {i} — Latent States Over Time\")\n",
        "        plt.imshow([z_sequences[i]], aspect='auto', cmap='tab20')\n",
        "        plt.xlabel(\"Time (frames)\")\n",
        "        plt.yticks([])\n",
        "        plt.colorbar(label=\"Latent State ID\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# === Call the visualization ===\n",
        "plot_latent_states(latent_state_sequences, num_videos=352)\n"
      ],
      "metadata": {
        "id": "V4j4wY--9fM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import groupby\n",
        "\n",
        "video_segments = []\n",
        "\n",
        "for seq in latent_state_sequences:\n",
        "    segments = []\n",
        "    for state, group in groupby(seq):\n",
        "        length = len(list(group))\n",
        "        segments.append((state, length))  # (state_id, duration)\n",
        "    video_segments.append(segments)\n",
        "\n",
        "# Save if needed\n",
        "with open(\"/content/drive/MyDrive/hmm_video_segments.pkl\", \"wb\") as f:\n",
        "    pickle.dump(video_segments, f)\n",
        "\n",
        "print(\"✅ Saved video-wise segmented latent state durations.\")\n"
      ],
      "metadata": {
        "id": "B8WIHJIH9fJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === Load latent state sequences ===\n",
        "with open(\"/content/drive/MyDrive/hmm_viterbi_latent_states.pkl\", \"rb\") as f:\n",
        "    latent_state_sequences = pickle.load(f)\n",
        "\n",
        "# === Load original video IDs (in same order as latent_state_sequences) ===\n",
        "with open(\"/content/drive/MyDrive/bphmm_input_observation_sequences.pkl\", \"rb\") as f:\n",
        "    video_observation_sequences = pickle.load(f)\n",
        "\n",
        "video_ids = list(video_observation_sequences.keys())  # 352 video filenames\n",
        "\n",
        "# === Sanity check ===\n",
        "assert len(video_ids) == len(latent_state_sequences), \"Mismatch in video count!\"\n",
        "\n",
        "# === Visualization function ===\n",
        "def plot_latent_states(z_sequences, video_ids, num_videos=5):\n",
        "    for i in range(min(num_videos, len(z_sequences))):\n",
        "        plt.figure(figsize=(10, 1.2))\n",
        "        plt.title(f\"{video_ids[i]} — Latent States Over Time\")\n",
        "        plt.imshow([z_sequences[i]], aspect='auto', cmap='tab20')\n",
        "        plt.xlabel(\"Time (frames)\")\n",
        "        plt.yticks([])\n",
        "        plt.colorbar(label=\"Latent State ID\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# === Call the plot ===\n",
        "plot_latent_states(latent_state_sequences, video_ids, num_videos=352)  # or use 352 to plot all\n"
      ],
      "metadata": {
        "id": "bPLwb9sT9fG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from itertools import groupby\n",
        "import os\n",
        "\n",
        "# === Load Viterbi latent states ===\n",
        "with open(\"/content/drive/MyDrive/hmm_viterbi_latent_states.pkl\", \"rb\") as f:\n",
        "    latent_state_sequences = pickle.load(f)\n",
        "\n",
        "# === Load original video IDs ===\n",
        "with open(\"/content/drive/MyDrive/bphmm_input_observation_sequences.pkl\", \"rb\") as f:\n",
        "    video_observation_sequences = pickle.load(f)\n",
        "\n",
        "video_ids = list(video_observation_sequences.keys())\n",
        "assert len(video_ids) == len(latent_state_sequences), \"Mismatch in video and sequence count.\"\n",
        "\n",
        "# === Compute top-3 longest runs of states per video ===\n",
        "video_top3_long_runs = {}\n",
        "\n",
        "for vid, z_seq in zip(video_ids, latent_state_sequences):\n",
        "    run_lengths = []\n",
        "\n",
        "    for state, group in groupby(z_seq):\n",
        "        run_len = len(list(group))\n",
        "        run_lengths.append((state, run_len))\n",
        "\n",
        "    # Get top 3 longest durations\n",
        "    top3 = sorted(run_lengths, key=lambda x: x[1], reverse=True)[:3]\n",
        "    top3_states = [s for s, _ in top3]\n",
        "    video_top3_long_runs[vid] = top3_states\n",
        "\n",
        "# === Save result ===\n",
        "save_path = \"/content/drive/MyDrive/hmm_video_top3_longest_run_states.pkl\"\n",
        "with open(save_path, \"wb\") as f:\n",
        "    pickle.dump(video_top3_long_runs, f)\n",
        "\n",
        "print(\"✅ Extracted top-3 continuous latent states per video (longest runs).\")\n",
        "print(f\"💾 Saved to: {save_path}\")\n"
      ],
      "metadata": {
        "id": "HxvqpQsm9fED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load your saved results\n",
        "with open(\"/content/drive/MyDrive/hmm_video_top3_longest_run_states.pkl\", \"rb\") as f:\n",
        "    video_top3_long_runs = pickle.load(f)\n",
        "\n",
        "# See how many videos were processed\n",
        "print(f\"✅ Loaded dominant state info for {len(video_top3_long_runs)} videos.\")\n"
      ],
      "metadata": {
        "id": "YVGJkXvR9fBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview results\n",
        "for i, (vid, top_states) in enumerate(video_top3_long_runs.items()):\n",
        "    print(f\"{vid}: Top-3 dominant states = {top_states}\")\n",
        "    if i == 352:\n",
        "        break  # Print first 10 only\n"
      ],
      "metadata": {
        "id": "PFjpupb59e-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_id = \"s0001_f_w000148\"  # replace with actual ID from your dataset\n",
        "if video_id in video_top3_long_runs:\n",
        "    print(f\"{video_id} → Top-3 dominant latent states: {video_top3_long_runs[video_id]}\")\n",
        "else:\n",
        "    print(\"⚠️ Video ID not found.\")\n"
      ],
      "metadata": {
        "id": "FmhPG24R9e8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count how often each state appears in any video's top-3\n",
        "state_counts = Counter()\n",
        "\n",
        "for top3 in video_top3_long_runs.values():\n",
        "    state_counts.update(top3)\n",
        "\n",
        "# Top 20 most frequent dominant states\n",
        "top_states = state_counts.most_common(20)\n",
        "states, counts = zip(*top_states)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.bar(states, counts, color=\"skyblue\")\n",
        "plt.xlabel(\"Latent State ID\")\n",
        "plt.ylabel(\"Number of Videos where it was in Top-3 Longest Run\")\n",
        "plt.title(\"🔢 Most Common Dominant States (Top-3 Longest Runs)\")\n",
        "plt.xticks(states)\n",
        "plt.grid(True, axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GR17NwuI9e5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SbJYeT4x9e2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bugtBzzi9ezP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dhrZOejv75E_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}